{"id": "0", "text": "# ColQwen Integration Guide\n\nEasy-to-use multimodal PDF retrieval with ColQwen2/ColPali models.\n\n## Quick Start\n\n> **üçé Mac Users**: ColQwen is optimized for Apple Silicon with MPS acceleration for faster inference!\n\n### 1. Install Dependencies\n```bash\nuv pip install colpali_engine pdf2image pillow matplotlib qwen_vl_utils einops seaborn\nbrew install poppler  # macOS only, for PDF processing\n```\n\n### 2. Basic Usage\n```bash\n# Build index from PDFs\npython -m apps.colqwen_rag build --pdfs ./my_papers/ --index research_papers\n\n# Search with text queries\npython -m apps.colqwen_rag search research_papers \"How does attention mechanism work?\"\n\n# Interactive Q&A\npython -m apps.colqwen_rag ask research_papers --interactive\n```\n\n## Commands", "metadata": {}}
{"id": "1", "text": "### 2. Basic Usage\n```bash\n# Build index from PDFs\npython -m apps.colqwen_rag build --pdfs ./my_papers/ --index research_papers\n\n# Search with text queries\npython -m apps.colqwen_rag search research_papers \"How does attention mechanism work?\"\n\n# Interactive Q&A\npython -m apps.colqwen_rag ask research_papers --interactive\n```\n\n## Commands\n\n### Build Index\n```bash\npython -m apps.colqwen_rag build \\\n  --pdfs ./pdf_directory/ \\\n  --index my_index \\\n  --model colqwen2 \\\n  --pages-dir ./page_images/  # Optional: save page images\n```\n\n**Options:**\n- `--pdfs`: Directory containing PDF files (or single PDF path)\n- `--index`: Name for the index (required)\n- `--model`: `colqwen2` (default) or `colpali`\n- `--pages-dir`: Directory to save page images (optional)", "metadata": {}}
{"id": "2", "text": "### Build Index\n```bash\npython -m apps.colqwen_rag build \\\n  --pdfs ./pdf_directory/ \\\n  --index my_index \\\n  --model colqwen2 \\\n  --pages-dir ./page_images/  # Optional: save page images\n```\n\n**Options:**\n- `--pdfs`: Directory containing PDF files (or single PDF path)\n- `--index`: Name for the index (required)\n- `--model`: `colqwen2` (default) or `colpali`\n- `--pages-dir`: Directory to save page images (optional)\n\n### Search Index\n```bash\npython -m apps.colqwen_rag search my_index \"your question here\" --top-k 5\n```\n\n**Options:**\n- `--top-k`: Number of results to return (default: 5)\n- `--model`: Model used for search (should match build model)\n\n### Interactive Q&A\n```bash\npython -m apps.colqwen_rag ask my_index --interactive\n```", "metadata": {}}
{"id": "3", "text": "### Search Index\n```bash\npython -m apps.colqwen_rag search my_index \"your question here\" --top-k 5\n```\n\n**Options:**\n- `--top-k`: Number of results to return (default: 5)\n- `--model`: Model used for search (should match build model)\n\n### Interactive Q&A\n```bash\npython -m apps.colqwen_rag ask my_index --interactive\n```\n\n**Commands in interactive mode:**\n- Type your questions naturally\n- `help`: Show available commands\n- `quit`/`exit`/`q`: Exit interactive mode\n\n## üß™ Test & Reproduce Results\n\nRun the reproduction test for issue #119:\n```bash\npython test_colqwen_reproduction.py\n```\n\nThis will:\n1. ‚úÖ Check dependencies\n2. üì• Download sample PDF (Attention Is All You Need paper)\n3. üèóÔ∏è Build test index\n4. üîç Run sample queries\n5. üìä Show how to generate similarity maps\n\n## üé® Advanced: Similarity Maps", "metadata": {}}
{"id": "4", "text": "## üß™ Test & Reproduce Results\n\nRun the reproduction test for issue #119:\n```bash\npython test_colqwen_reproduction.py\n```\n\nThis will:\n1. ‚úÖ Check dependencies\n2. üì• Download sample PDF (Attention Is All You Need paper)\n3. üèóÔ∏è Build test index\n4. üîç Run sample queries\n5. üìä Show how to generate similarity maps\n\n## üé® Advanced: Similarity Maps\n\nFor visual similarity analysis, use the existing advanced script:\n```bash\ncd apps/multimodal/vision-based-pdf-multi-vector/\npython multi-vector-leann-similarity-map.py\n```\n\nEdit the script to customize:\n- `QUERY`: Your question\n- `MODEL`: \"colqwen2\" or \"colpali\"\n- `USE_HF_DATASET`: Use HuggingFace dataset or local PDFs\n- `SIMILARITY_MAP`: Generate heatmaps\n- `ANSWER`: Enable Qwen-VL answer generation\n\n## üîß How It Works", "metadata": {}}
{"id": "5", "text": "For visual similarity analysis, use the existing advanced script:\n```bash\ncd apps/multimodal/vision-based-pdf-multi-vector/\npython multi-vector-leann-similarity-map.py\n```\n\nEdit the script to customize:\n- `QUERY`: Your question\n- `MODEL`: \"colqwen2\" or \"colpali\"\n- `USE_HF_DATASET`: Use HuggingFace dataset or local PDFs\n- `SIMILARITY_MAP`: Generate heatmaps\n- `ANSWER`: Enable Qwen-VL answer generation\n\n## üîß How It Works\n\n### ColQwen2 vs ColPali\n- **ColQwen2** (`vidore/colqwen2-v1.0`): Latest vision-language model\n- **ColPali** (`vidore/colpali-v1.2`): Proven multimodal retriever", "metadata": {}}
{"id": "6", "text": "## üîß How It Works\n\n### ColQwen2 vs ColPali\n- **ColQwen2** (`vidore/colqwen2-v1.0`): Latest vision-language model\n- **ColPali** (`vidore/colpali-v1.2`): Proven multimodal retriever\n\n### Architecture\n1. **PDF ‚Üí Images**: Convert PDF pages to images (150 DPI)\n2. **Vision Encoding**: Process images with ColQwen2/ColPali\n3. **Multi-Vector Index**: Build LEANN HNSW index with multiple embeddings per page\n4. **Query Processing**: Encode text queries with same model\n5. **Similarity Search**: Find most relevant pages/regions\n6. **Visual Maps**: Generate attention heatmaps (optional)\n\n### Device Support\n- **CUDA**: Best performance with GPU acceleration\n- **MPS**: Apple Silicon Mac support\n- **CPU**: Fallback for any system (slower)\n\nAuto-detection: CUDA > MPS > CPU\n\n## üìä Performance Tips", "metadata": {}}
{"id": "7", "text": "### Device Support\n- **CUDA**: Best performance with GPU acceleration\n- **MPS**: Apple Silicon Mac support\n- **CPU**: Fallback for any system (slower)\n\nAuto-detection: CUDA > MPS > CPU\n\n## üìä Performance Tips\n\n### For Best Performance:\n```bash\n# Use ColQwen2 for latest features\n--model colqwen2\n\n# Save page images for reuse\n--pages-dir ./cached_pages/\n\n# Adjust batch size based on GPU memory\n# (automatically handled)\n```\n\n### For Large Document Sets:\n- Process PDFs in batches\n- Use SSD storage for index files\n- Consider using CUDA if available\n\n## üîó Related Resources", "metadata": {}}
{"id": "8", "text": "Auto-detection: CUDA > MPS > CPU\n\n## üìä Performance Tips\n\n### For Best Performance:\n```bash\n# Use ColQwen2 for latest features\n--model colqwen2\n\n# Save page images for reuse\n--pages-dir ./cached_pages/\n\n# Adjust batch size based on GPU memory\n# (automatically handled)\n```\n\n### For Large Document Sets:\n- Process PDFs in batches\n- Use SSD storage for index files\n- Consider using CUDA if available\n\n## üîó Related Resources\n\n- **Fast-PLAID**: https://github.com/lightonai/fast-plaid\n- **Pylate**: https://github.com/lightonai/pylate\n- **ColBERT**: https://github.com/stanford-futuredata/ColBERT\n- **ColPali Paper**: Vision-Language Models for Document Retrieval\n- **Issue #119**: https://github.com/yichuan-w/LEANN/issues/119\n\n## üêõ Troubleshooting", "metadata": {}}
{"id": "9", "text": "## üîó Related Resources\n\n- **Fast-PLAID**: https://github.com/lightonai/fast-plaid\n- **Pylate**: https://github.com/lightonai/pylate\n- **ColBERT**: https://github.com/stanford-futuredata/ColBERT\n- **ColPali Paper**: Vision-Language Models for Document Retrieval\n- **Issue #119**: https://github.com/yichuan-w/LEANN/issues/119\n\n## üêõ Troubleshooting\n\n### PDF Conversion Issues (macOS)\n```bash\n# Install poppler\nbrew install poppler\nwhich pdfinfo && pdfinfo -v\n```\n\n### Memory Issues\n- Reduce batch size (automatically handled)\n- Use CPU instead of GPU: `export CUDA_VISIBLE_DEVICES=\"\"`\n- Process fewer PDFs at once\n\n### Model Download Issues\n- Ensure internet connection for first run\n- Models are cached after first download\n- Use HuggingFace mirrors if needed", "metadata": {}}
{"id": "10", "text": "## üêõ Troubleshooting\n\n### PDF Conversion Issues (macOS)\n```bash\n# Install poppler\nbrew install poppler\nwhich pdfinfo && pdfinfo -v\n```\n\n### Memory Issues\n- Reduce batch size (automatically handled)\n- Use CPU instead of GPU: `export CUDA_VISIBLE_DEVICES=\"\"`\n- Process fewer PDFs at once\n\n### Model Download Issues\n- Ensure internet connection for first run\n- Models are cached after first download\n- Use HuggingFace mirrors if needed\n\n### Import Errors\n```bash\n# Ensure all dependencies installed\nuv pip install colpali_engine pdf2image pillow matplotlib qwen_vl_utils einops seaborn\n\n# Check PyTorch installation\npython -c \"import torch; print(torch.__version__)\"\n```\n\n## üí° Examples\n\n### Research Paper Analysis\n```bash\n# Index your research papers\npython -m apps.colqwen_rag build --pdfs ~/Papers/AI/ --index ai_papers", "metadata": {}}
{"id": "11", "text": "### Import Errors\n```bash\n# Ensure all dependencies installed\nuv pip install colpali_engine pdf2image pillow matplotlib qwen_vl_utils einops seaborn\n\n# Check PyTorch installation\npython -c \"import torch; print(torch.__version__)\"\n```\n\n## üí° Examples\n\n### Research Paper Analysis\n```bash\n# Index your research papers\npython -m apps.colqwen_rag build --pdfs ~/Papers/AI/ --index ai_papers\n\n# Ask research questions\npython -m apps.colqwen_rag search ai_papers \"What are the limitations of transformer models?\"\npython -m apps.colqwen_rag search ai_papers \"How does BERT compare to GPT?\"\n```\n\n### Document Q&A\n```bash\n# Index business documents\npython -m apps.colqwen_rag build --pdfs ~/Documents/Reports/ --index reports\n\n# Interactive analysis\npython -m apps.colqwen_rag ask reports --interactive\n```", "metadata": {}}
{"id": "12", "text": "# Ask research questions\npython -m apps.colqwen_rag search ai_papers \"What are the limitations of transformer models?\"\npython -m apps.colqwen_rag search ai_papers \"How does BERT compare to GPT?\"\n```\n\n### Document Q&A\n```bash\n# Index business documents\npython -m apps.colqwen_rag build --pdfs ~/Documents/Reports/ --index reports\n\n# Interactive analysis\npython -m apps.colqwen_rag ask reports --interactive\n```\n\n### Visual Analysis\n```bash\n# Generate similarity maps for specific queries\ncd apps/multimodal/vision-based-pdf-multi-vector/\n# Edit multi-vector-leann-similarity-map.py with your query\npython multi-vector-leann-similarity-map.py\n# Check ./figures/ for generated heatmaps\n```\n\n---\n\n**üéØ This integration makes ColQwen as easy to use as other LEANN features while maintaining the full power of multimodal document understanding!**", "metadata": {}}
{"id": "13", "text": "# ü§ù Contributing\n\nWe welcome contributions! Leann is built by the community, for the community.\n\n## Ways to Contribute\n\n- üêõ **Bug Reports**: Found an issue? Let us know!\n- üí° **Feature Requests**: Have an idea? We'd love to hear it!\n- üîß **Code Contributions**: PRs welcome for all skill levels\n- üìñ **Documentation**: Help make Leann more accessible\n- üß™ **Benchmarks**: Share your performance results\n\n## üöÄ Development Setup\n\n### Prerequisites\n\n1. **Install uv** (fast Python package installer):\n   ```bash\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n   ```\n\n2. **Clone the repository**:\n   ```bash\n   git clone https://github.com/yichuan-w/LEANN.git leann\n   cd leann\n   ```\n\n3. **Install system dependencies**:\n\n   **macOS:**\n   ```bash\n   brew install llvm libomp boost protobuf zeromq pkgconf\n   ```", "metadata": {}}
{"id": "14", "text": "## üöÄ Development Setup\n\n### Prerequisites\n\n1. **Install uv** (fast Python package installer):\n   ```bash\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n   ```\n\n2. **Clone the repository**:\n   ```bash\n   git clone https://github.com/yichuan-w/LEANN.git leann\n   cd leann\n   ```\n\n3. **Install system dependencies**:\n\n   **macOS:**\n   ```bash\n   brew install llvm libomp boost protobuf zeromq pkgconf\n   ```\n\n   **Ubuntu/Debian:**\n   ```bash\n   sudo apt-get install libomp-dev libboost-all-dev protobuf-compiler \\\n                        libabsl-dev libmkl-full-dev libaio-dev libzmq3-dev\n   ```\n\n4. **Build from source**:\n   ```bash\n   # macOS\n   CC=$(brew --prefix llvm)/bin/clang CXX=$(brew --prefix llvm)/bin/clang++ uv sync\n\n   # Ubuntu/Debian\n   uv sync\n   ```", "metadata": {}}
{"id": "15", "text": "**Ubuntu/Debian:**\n   ```bash\n   sudo apt-get install libomp-dev libboost-all-dev protobuf-compiler \\\n                        libabsl-dev libmkl-full-dev libaio-dev libzmq3-dev\n   ```\n\n4. **Build from source**:\n   ```bash\n   # macOS\n   CC=$(brew --prefix llvm)/bin/clang CXX=$(brew --prefix llvm)/bin/clang++ uv sync\n\n   # Ubuntu/Debian\n   uv sync\n   ```\n\n## üî® Pre-commit Hooks\n\nWe use pre-commit hooks to ensure code quality and consistency. This runs automatically before each commit.\n\n### Setup Pre-commit\n\n1. **Install pre-commit tools**:\n   ```bash\n   uv sync lint\n   ```\n\n2. **Install the git hooks**:\n   ```bash\n   pre-commit install\n   ```\n\n3. **Run pre-commit manually** (optional):\n   ```bash\n   uv run pre-commit run --all-files\n   ```\n\n### Pre-commit Checks", "metadata": {}}
{"id": "16", "text": "# Ubuntu/Debian\n   uv sync\n   ```\n\n## üî® Pre-commit Hooks\n\nWe use pre-commit hooks to ensure code quality and consistency. This runs automatically before each commit.\n\n### Setup Pre-commit\n\n1. **Install pre-commit tools**:\n   ```bash\n   uv sync lint\n   ```\n\n2. **Install the git hooks**:\n   ```bash\n   pre-commit install\n   ```\n\n3. **Run pre-commit manually** (optional):\n   ```bash\n   uv run pre-commit run --all-files\n   ```\n\n### Pre-commit Checks\n\nOur pre-commit configuration includes:\n- **Trailing whitespace removal**\n- **End-of-file fixing**\n- **YAML validation**\n- **Large file prevention**\n- **Merge conflict detection**\n- **Debug statement detection**\n- **Code formatting with ruff**\n- **Code linting with ruff**\n\n## üß™ Testing\n\n### Running Tests\n\n```bash\n# Install test tools only (no project runtime)\nuv sync --group test\n\n# Run all tests\nuv run pytest", "metadata": {}}
{"id": "17", "text": "### Pre-commit Checks\n\nOur pre-commit configuration includes:\n- **Trailing whitespace removal**\n- **End-of-file fixing**\n- **YAML validation**\n- **Large file prevention**\n- **Merge conflict detection**\n- **Debug statement detection**\n- **Code formatting with ruff**\n- **Code linting with ruff**\n\n## üß™ Testing\n\n### Running Tests\n\n```bash\n# Install test tools only (no project runtime)\nuv sync --group test\n\n# Run all tests\nuv run pytest\n\n# Run specific test file\nuv run pytest test/test_filename.py\n\n# Run with coverage\nuv run pytest --cov=leann\n```\n\n### Writing Tests\n\n- Place tests in the `test/` directory\n- Follow the naming convention `test_*.py`\n- Use descriptive test names that explain what's being tested\n- Include both positive and negative test cases\n\n## üìù Code Style\n\nWe use `ruff` for both linting and formatting to ensure consistent code style.\n\n### Format Your Code\n\n```bash\n# Format all files\nruff format", "metadata": {}}
{"id": "18", "text": "# Run specific test file\nuv run pytest test/test_filename.py\n\n# Run with coverage\nuv run pytest --cov=leann\n```\n\n### Writing Tests\n\n- Place tests in the `test/` directory\n- Follow the naming convention `test_*.py`\n- Use descriptive test names that explain what's being tested\n- Include both positive and negative test cases\n\n## üìù Code Style\n\nWe use `ruff` for both linting and formatting to ensure consistent code style.\n\n### Format Your Code\n\n```bash\n# Format all files\nruff format\n\n# Check formatting without changing files\nruff format --check\n```\n\n### Lint Your Code\n\n```bash\n# Run linter with auto-fix\nruff check --fix\n\n# Just check without fixing\nruff check\n```\n\n### Style Guidelines\n\n- Follow PEP 8 conventions\n- Use descriptive variable names\n- Add type hints where appropriate\n- Write docstrings for all public functions and classes\n- Keep functions focused and single-purpose\n\n## üö¶ CI/CD", "metadata": {}}
{"id": "19", "text": "### Format Your Code\n\n```bash\n# Format all files\nruff format\n\n# Check formatting without changing files\nruff format --check\n```\n\n### Lint Your Code\n\n```bash\n# Run linter with auto-fix\nruff check --fix\n\n# Just check without fixing\nruff check\n```\n\n### Style Guidelines\n\n- Follow PEP 8 conventions\n- Use descriptive variable names\n- Add type hints where appropriate\n- Write docstrings for all public functions and classes\n- Keep functions focused and single-purpose\n\n## üö¶ CI/CD\n\nOur CI pipeline runs automatically on all pull requests. It includes:\n\n1. **Linting and Formatting**: Ensures code follows our style guidelines\n2. **Multi-platform builds**: Tests on Ubuntu and macOS\n3. **Python version matrix**: Tests on Python 3.9-3.13\n4. **Wheel building**: Ensures packages can be built and distributed\n\n### CI Commands\n\nThe CI uses the same commands as pre-commit to ensure consistency:\n```bash\n# Linting\nruff check .", "metadata": {}}
{"id": "20", "text": "## üö¶ CI/CD\n\nOur CI pipeline runs automatically on all pull requests. It includes:\n\n1. **Linting and Formatting**: Ensures code follows our style guidelines\n2. **Multi-platform builds**: Tests on Ubuntu and macOS\n3. **Python version matrix**: Tests on Python 3.9-3.13\n4. **Wheel building**: Ensures packages can be built and distributed\n\n### CI Commands\n\nThe CI uses the same commands as pre-commit to ensure consistency:\n```bash\n# Linting\nruff check .\n\n# Format checking\nruff format --check .\n```\n\nMake sure your code passes these checks locally before pushing!\n\n## üîÑ Pull Request Process\n\n1. **Fork the repository** and create your branch from `main`:\n   ```bash\n   git checkout -b feature/your-feature-name\n   ```\n\n2. **Make your changes**:\n   - Write clean, documented code\n   - Add tests for new functionality\n   - Update documentation as needed", "metadata": {}}
{"id": "21", "text": "### CI Commands\n\nThe CI uses the same commands as pre-commit to ensure consistency:\n```bash\n# Linting\nruff check .\n\n# Format checking\nruff format --check .\n```\n\nMake sure your code passes these checks locally before pushing!\n\n## üîÑ Pull Request Process\n\n1. **Fork the repository** and create your branch from `main`:\n   ```bash\n   git checkout -b feature/your-feature-name\n   ```\n\n2. **Make your changes**:\n   - Write clean, documented code\n   - Add tests for new functionality\n   - Update documentation as needed\n\n3. **Run pre-commit checks**:\n   ```bash\n   pre-commit run --all-files\n   ```\n\n4. **Test your changes**:\n   ```bash\n   uv run pytest\n   ```\n\n5. **Commit with descriptive messages**:\n   ```bash\n   git commit -m \"feat: add new search algorithm\"\n   ```", "metadata": {}}
{"id": "22", "text": "2. **Make your changes**:\n   - Write clean, documented code\n   - Add tests for new functionality\n   - Update documentation as needed\n\n3. **Run pre-commit checks**:\n   ```bash\n   pre-commit run --all-files\n   ```\n\n4. **Test your changes**:\n   ```bash\n   uv run pytest\n   ```\n\n5. **Commit with descriptive messages**:\n   ```bash\n   git commit -m \"feat: add new search algorithm\"\n   ```\n\n   Follow [Conventional Commits](https://www.conventionalcommits.org/):\n   - `feat:` for new features\n   - `fix:` for bug fixes\n   - `docs:` for documentation changes\n   - `test:` for test additions/changes\n   - `refactor:` for code refactoring\n   - `perf:` for performance improvements\n\n6. **Push and create a pull request**:\n   - Provide a clear description of your changes\n   - Reference any related issues\n   - Include examples or screenshots if applicable\n\n## üìö Documentation\n\nWhen adding new features or making significant changes:", "metadata": {}}
{"id": "23", "text": "6. **Push and create a pull request**:\n   - Provide a clear description of your changes\n   - Reference any related issues\n   - Include examples or screenshots if applicable\n\n## üìö Documentation\n\nWhen adding new features or making significant changes:\n\n1. Update relevant documentation in `/docs`\n2. Add docstrings to new functions/classes\n3. Update README.md if needed\n4. Include usage examples\n\n## ü§î Getting Help\n\n- **Discord**: Join our community for discussions\n- **Issues**: Check existing issues or create a new one\n- **Discussions**: For general questions and ideas\n\n## üìÑ License\n\nBy contributing, you agree that your contributions will be licensed under the same license as the project (MIT).\n\n---\n\nThank you for contributing to LEANN! Every contribution, no matter how small, helps make the project better for everyone. üåü", "metadata": {}}
{"id": "24", "text": "# Release Guide\n\n## Setup (One-time)\n\nAdd `PYPI_API_TOKEN` to GitHub Secrets:\n1. Get token: https://pypi.org/manage/account/token/\n2. Add to secrets: Settings ‚Üí Secrets ‚Üí Actions ‚Üí `PYPI_API_TOKEN`\n\n## Release (One-click)\n\n1. Go to: https://github.com/yichuan-w/LEANN/actions/workflows/release-manual.yml\n2. Click \"Run workflow\"\n3. Enter version: `0.1.2`\n4. Click green \"Run workflow\" button\n\nThat's it! The workflow will automatically:\n- ‚úÖ Update version in all packages\n- ‚úÖ Build all packages\n- ‚úÖ Publish to PyPI\n- ‚úÖ Create GitHub tag and release\n\nCheck progress: https://github.com/yichuan-w/LEANN/actions", "metadata": {}}
{"id": "25", "text": "# Thinking Budget Feature Implementation\n\n## Overview\n\nThis document describes the implementation of the **thinking budget** feature for LEANN, which allows users to control the computational effort for reasoning models like GPT-Oss:20b.\n\n## Feature Description\n\nThe thinking budget feature provides three levels of computational effort for reasoning models:\n- **`low`**: Fast responses, basic reasoning (default for simple queries)\n- **`medium`**: Balanced speed and reasoning depth\n- **`high`**: Maximum reasoning effort, best for complex analytical questions\n\n## Implementation Details\n\n### 1. Command Line Interface\n\nAdded `--thinking-budget` parameter to both CLI and RAG examples:\n\n```bash\n# LEANN CLI\nleann ask my-index --llm ollama --model gpt-oss:20b --thinking-budget high\n\n# RAG Examples\npython apps/email_rag.py --llm ollama --llm-model gpt-oss:20b --thinking-budget high\npython apps/document_rag.py --llm openai --llm-model o3 --thinking-budget medium\n```", "metadata": {}}
{"id": "26", "text": "## Implementation Details\n\n### 1. Command Line Interface\n\nAdded `--thinking-budget` parameter to both CLI and RAG examples:\n\n```bash\n# LEANN CLI\nleann ask my-index --llm ollama --model gpt-oss:20b --thinking-budget high\n\n# RAG Examples\npython apps/email_rag.py --llm ollama --llm-model gpt-oss:20b --thinking-budget high\npython apps/document_rag.py --llm openai --llm-model o3 --thinking-budget medium\n```\n\n### 2. LLM Backend Support\n\n#### Ollama Backend (`packages/leann-core/src/leann/chat.py`)", "metadata": {}}
{"id": "27", "text": "```bash\n# LEANN CLI\nleann ask my-index --llm ollama --model gpt-oss:20b --thinking-budget high\n\n# RAG Examples\npython apps/email_rag.py --llm ollama --llm-model gpt-oss:20b --thinking-budget high\npython apps/document_rag.py --llm openai --llm-model o3 --thinking-budget medium\n```\n\n### 2. LLM Backend Support\n\n#### Ollama Backend (`packages/leann-core/src/leann/chat.py`)\n\n```python\ndef ask(self, prompt: str, **kwargs) -> str:\n    # Handle thinking budget for reasoning models\n    options = kwargs.copy()\n    thinking_budget = kwargs.get(\"thinking_budget\")\n    if thinking_budget:\n        options.pop(\"thinking_budget\", None)\n        if thinking_budget in [\"low\", \"medium\", \"high\"]:\n            options[\"reasoning\"] = {\"effort\": thinking_budget, \"exclude\": False}\n```", "metadata": {}}
{"id": "28", "text": "### 2. LLM Backend Support\n\n#### Ollama Backend (`packages/leann-core/src/leann/chat.py`)\n\n```python\ndef ask(self, prompt: str, **kwargs) -> str:\n    # Handle thinking budget for reasoning models\n    options = kwargs.copy()\n    thinking_budget = kwargs.get(\"thinking_budget\")\n    if thinking_budget:\n        options.pop(\"thinking_budget\", None)\n        if thinking_budget in [\"low\", \"medium\", \"high\"]:\n            options[\"reasoning\"] = {\"effort\": thinking_budget, \"exclude\": False}\n```\n\n**API Format**: Uses Ollama's `reasoning` parameter with `effort` and `exclude` fields.\n\n#### OpenAI Backend (`packages/leann-core/src/leann/chat.py`)", "metadata": {}}
{"id": "29", "text": "**API Format**: Uses Ollama's `reasoning` parameter with `effort` and `exclude` fields.\n\n#### OpenAI Backend (`packages/leann-core/src/leann/chat.py`)\n\n```python\ndef ask(self, prompt: str, **kwargs) -> str:\n    # Handle thinking budget for reasoning models\n    thinking_budget = kwargs.get(\"thinking_budget\")\n    if thinking_budget and thinking_budget in [\"low\", \"medium\", \"high\"]:\n        # Check if this is an o-series model\n        o_series_models = [\"o3\", \"o3-mini\", \"o4-mini\", \"o1\", \"o3-pro\", \"o3-deep-research\"]\n        if any(model in self.model for model in o_series_models):\n            params[\"reasoning_effort\"] = thinking_budget\n```\n\n**API Format**: Uses OpenAI's `reasoning_effort` parameter for o-series models.\n\n### 3. Parameter Propagation\n\nThe thinking budget parameter is properly propagated through the LEANN architecture:", "metadata": {}}
{"id": "30", "text": "**API Format**: Uses OpenAI's `reasoning_effort` parameter for o-series models.\n\n### 3. Parameter Propagation\n\nThe thinking budget parameter is properly propagated through the LEANN architecture:\n\n1. **CLI** (`packages/leann-core/src/leann/cli.py`): Captures `--thinking-budget` argument\n2. **Base RAG** (`apps/base_rag_example.py`): Adds parameter to argument parser\n3. **LeannChat** (`packages/leann-core/src/leann/api.py`): Passes `llm_kwargs` to LLM\n4. **LLM Interface**: Handles the parameter in backend-specific implementations\n\n## Files Modified\n\n### Core Implementation\n- `packages/leann-core/src/leann/chat.py`: Added thinking budget support to OllamaChat and OpenAIChat\n- `packages/leann-core/src/leann/cli.py`: Added `--thinking-budget` argument\n- `apps/base_rag_example.py`: Added thinking budget parameter to RAG examples", "metadata": {}}
{"id": "31", "text": "## Files Modified\n\n### Core Implementation\n- `packages/leann-core/src/leann/chat.py`: Added thinking budget support to OllamaChat and OpenAIChat\n- `packages/leann-core/src/leann/cli.py`: Added `--thinking-budget` argument\n- `apps/base_rag_example.py`: Added thinking budget parameter to RAG examples\n\n### Documentation\n- `README.md`: Added thinking budget parameter to usage examples\n- `docs/configuration-guide.md`: Added detailed documentation and usage guidelines\n\n### Examples\n- `examples/thinking_budget_demo.py`: Comprehensive demo script with usage examples\n\n## Usage Examples\n\n### Basic Usage\n```bash\n# High reasoning effort for complex questions\nleann ask my-index --llm ollama --model gpt-oss:20b --thinking-budget high\n\n# Medium reasoning for balanced performance\nleann ask my-index --llm openai --model gpt-4o --thinking-budget medium", "metadata": {}}
{"id": "32", "text": "### Documentation\n- `README.md`: Added thinking budget parameter to usage examples\n- `docs/configuration-guide.md`: Added detailed documentation and usage guidelines\n\n### Examples\n- `examples/thinking_budget_demo.py`: Comprehensive demo script with usage examples\n\n## Usage Examples\n\n### Basic Usage\n```bash\n# High reasoning effort for complex questions\nleann ask my-index --llm ollama --model gpt-oss:20b --thinking-budget high\n\n# Medium reasoning for balanced performance\nleann ask my-index --llm openai --model gpt-4o --thinking-budget medium\n\n# Low reasoning for fast responses\nleann ask my-index --llm ollama --model gpt-oss:20b --thinking-budget low\n```\n\n### RAG Examples\n```bash\n# Email RAG with high reasoning\npython apps/email_rag.py --llm ollama --llm-model gpt-oss:20b --thinking-budget high", "metadata": {}}
{"id": "33", "text": "# Medium reasoning for balanced performance\nleann ask my-index --llm openai --model gpt-4o --thinking-budget medium\n\n# Low reasoning for fast responses\nleann ask my-index --llm ollama --model gpt-oss:20b --thinking-budget low\n```\n\n### RAG Examples\n```bash\n# Email RAG with high reasoning\npython apps/email_rag.py --llm ollama --llm-model gpt-oss:20b --thinking-budget high\n\n# Document RAG with medium reasoning\npython apps/document_rag.py --llm openai --llm-model gpt-4o --thinking-budget medium\n```\n\n## Supported Models\n\n### Ollama Models\n- **GPT-Oss:20b**: Primary target model with reasoning capabilities\n- **Other reasoning models**: Any Ollama model that supports the `reasoning` parameter", "metadata": {}}
{"id": "34", "text": "### RAG Examples\n```bash\n# Email RAG with high reasoning\npython apps/email_rag.py --llm ollama --llm-model gpt-oss:20b --thinking-budget high\n\n# Document RAG with medium reasoning\npython apps/document_rag.py --llm openai --llm-model gpt-4o --thinking-budget medium\n```\n\n## Supported Models\n\n### Ollama Models\n- **GPT-Oss:20b**: Primary target model with reasoning capabilities\n- **Other reasoning models**: Any Ollama model that supports the `reasoning` parameter\n\n### OpenAI Models\n- **o3, o3-mini, o4-mini, o1**: o-series reasoning models with `reasoning_effort` parameter\n- **GPT-OSS models**: Models that support reasoning capabilities\n\n## Testing\n\nThe implementation includes comprehensive testing:\n- Parameter handling verification\n- Backend-specific API format validation\n- CLI argument parsing tests\n- Integration with existing LEANN architecture", "metadata": {}}
{"id": "35", "text": "# AST-Aware Code chunking guide\n\n## Overview\n\nThis guide covers best practices for using AST-aware code chunking in LEANN. AST chunking provides better semantic understanding of code structure compared to traditional text-based chunking.\n\n## Quick Start\n\n### Basic Usage\n\n```bash\n# Enable AST chunking for mixed content (code + docs)\npython -m apps.document_rag --enable-code-chunking --data-dir ./my_project\n\n# Specialized code repository indexing\npython -m apps.code_rag --repo-dir ./my_codebase\n\n# Global CLI with AST support\nleann build my-code-index --docs ./src --use-ast-chunking\n```\n\n### Installation\n\n```bash\n# Install LEANN with AST chunking support\nuv pip install -e \".\"\n```\n\n#### For normal users (PyPI install)\n- Use `pip install leann` or `uv pip install leann`.\n- `astchunk` is pulled automatically from PyPI as a dependency; no extra steps.", "metadata": {}}
{"id": "36", "text": "# Specialized code repository indexing\npython -m apps.code_rag --repo-dir ./my_codebase\n\n# Global CLI with AST support\nleann build my-code-index --docs ./src --use-ast-chunking\n```\n\n### Installation\n\n```bash\n# Install LEANN with AST chunking support\nuv pip install -e \".\"\n```\n\n#### For normal users (PyPI install)\n- Use `pip install leann` or `uv pip install leann`.\n- `astchunk` is pulled automatically from PyPI as a dependency; no extra steps.\n\n#### For developers (from source, editable)\n```bash\ngit clone https://github.com/yichuan-w/LEANN.git leann\ncd leann\ngit submodule update --init --recursive\nuv sync\n```\n- This repo vendors `astchunk` as a git submodule at `packages/astchunk-leann` (our fork).\n- `[tool.uv.sources]` maps the `astchunk` package to that path in editable mode.\n- You can edit code under `packages/astchunk-leann` and Python will use your changes immediately (no separate `pip install astchunk` needed).", "metadata": {}}
{"id": "37", "text": "#### For developers (from source, editable)\n```bash\ngit clone https://github.com/yichuan-w/LEANN.git leann\ncd leann\ngit submodule update --init --recursive\nuv sync\n```\n- This repo vendors `astchunk` as a git submodule at `packages/astchunk-leann` (our fork).\n- `[tool.uv.sources]` maps the `astchunk` package to that path in editable mode.\n- You can edit code under `packages/astchunk-leann` and Python will use your changes immediately (no separate `pip install astchunk` needed).\n\n## Best Practices\n\n### When to Use AST Chunking\n\n‚úÖ **Recommended for:**\n- Code repositories with multiple languages\n- Mixed documentation and code content\n- Complex codebases with deep function/class hierarchies\n- When working with Claude Code for code assistance\n\n‚ùå **Not recommended for:**\n- Pure text documents\n- Very large files (>1MB)\n- Languages not supported by tree-sitter\n\n### Optimal Configuration", "metadata": {}}
{"id": "38", "text": "## Best Practices\n\n### When to Use AST Chunking\n\n‚úÖ **Recommended for:**\n- Code repositories with multiple languages\n- Mixed documentation and code content\n- Complex codebases with deep function/class hierarchies\n- When working with Claude Code for code assistance\n\n‚ùå **Not recommended for:**\n- Pure text documents\n- Very large files (>1MB)\n- Languages not supported by tree-sitter\n\n### Optimal Configuration\n\n```bash\n# Recommended settings for most codebases\npython -m apps.code_rag \\\n    --repo-dir ./src \\\n    --ast-chunk-size 768 \\\n    --ast-chunk-overlap 96 \\\n    --exclude-dirs .git __pycache__ node_modules build dist\n```\n\n### Supported Languages", "metadata": {}}
{"id": "39", "text": "‚ùå **Not recommended for:**\n- Pure text documents\n- Very large files (>1MB)\n- Languages not supported by tree-sitter\n\n### Optimal Configuration\n\n```bash\n# Recommended settings for most codebases\npython -m apps.code_rag \\\n    --repo-dir ./src \\\n    --ast-chunk-size 768 \\\n    --ast-chunk-overlap 96 \\\n    --exclude-dirs .git __pycache__ node_modules build dist\n```\n\n### Supported Languages\n\n| Extension | Language | Status |\n|-----------|----------|--------|\n| `.py` | Python | ‚úÖ Full support |\n| `.java` | Java | ‚úÖ Full support |\n| `.cs` | C# | ‚úÖ Full support |\n| `.ts`, `.tsx` | TypeScript | ‚úÖ Full support |\n| `.js`, `.jsx` | JavaScript | ‚úÖ Via TypeScript parser |\n\n## Integration Examples\n\n### Document RAG with Code Support", "metadata": {}}
{"id": "40", "text": "### Supported Languages\n\n| Extension | Language | Status |\n|-----------|----------|--------|\n| `.py` | Python | ‚úÖ Full support |\n| `.java` | Java | ‚úÖ Full support |\n| `.cs` | C# | ‚úÖ Full support |\n| `.ts`, `.tsx` | TypeScript | ‚úÖ Full support |\n| `.js`, `.jsx` | JavaScript | ‚úÖ Via TypeScript parser |\n\n## Integration Examples\n\n### Document RAG with Code Support\n\n```python\n# Enable code chunking in document RAG\npython -m apps.document_rag \\\n    --enable-code-chunking \\\n    --data-dir ./project \\\n    --query \"How does authentication work in the codebase?\"\n```\n\n### Claude Code Integration\n\nWhen using with Claude Code MCP server, AST chunking provides better context for:\n- Code completion and suggestions\n- Bug analysis and debugging\n- Architecture understanding\n- Refactoring assistance\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Fallback to Traditional Chunking**\n   - Normal behavior for unsupported languages\n   - Check logs for specific language support", "metadata": {}}
{"id": "41", "text": "```python\n# Enable code chunking in document RAG\npython -m apps.document_rag \\\n    --enable-code-chunking \\\n    --data-dir ./project \\\n    --query \"How does authentication work in the codebase?\"\n```\n\n### Claude Code Integration\n\nWhen using with Claude Code MCP server, AST chunking provides better context for:\n- Code completion and suggestions\n- Bug analysis and debugging\n- Architecture understanding\n- Refactoring assistance\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Fallback to Traditional Chunking**\n   - Normal behavior for unsupported languages\n   - Check logs for specific language support\n\n2. **Performance with Large Files**\n   - Adjust `--max-file-size` parameter\n   - Use `--exclude-dirs` to skip unnecessary directories\n\n3. **Quality Issues**\n   - Try different `--ast-chunk-size` values (512, 768, 1024)\n   - Adjust overlap for better context preservation\n\n### Debug Mode\n\n```bash\nexport LEANN_LOG_LEVEL=DEBUG\npython -m apps.code_rag --repo-dir ./my_code\n```", "metadata": {}}
{"id": "42", "text": "1. **Fallback to Traditional Chunking**\n   - Normal behavior for unsupported languages\n   - Check logs for specific language support\n\n2. **Performance with Large Files**\n   - Adjust `--max-file-size` parameter\n   - Use `--exclude-dirs` to skip unnecessary directories\n\n3. **Quality Issues**\n   - Try different `--ast-chunk-size` values (512, 768, 1024)\n   - Adjust overlap for better context preservation\n\n### Debug Mode\n\n```bash\nexport LEANN_LOG_LEVEL=DEBUG\npython -m apps.code_rag --repo-dir ./my_code\n```\n\n## Migration from Traditional Chunking\n\nExisting workflows continue to work without changes. To enable AST chunking:\n\n```bash\n# Before\npython -m apps.document_rag --chunk-size 256\n\n# After (maintains traditional chunking for non-code files)\npython -m apps.document_rag --enable-code-chunking --chunk-size 256 --ast-chunk-size 768\n```\n\n## References", "metadata": {}}
{"id": "43", "text": "### Debug Mode\n\n```bash\nexport LEANN_LOG_LEVEL=DEBUG\npython -m apps.code_rag --repo-dir ./my_code\n```\n\n## Migration from Traditional Chunking\n\nExisting workflows continue to work without changes. To enable AST chunking:\n\n```bash\n# Before\npython -m apps.document_rag --chunk-size 256\n\n# After (maintains traditional chunking for non-code files)\npython -m apps.document_rag --enable-code-chunking --chunk-size 256 --ast-chunk-size 768\n```\n\n## References\n\n- [astchunk GitHub Repository](https://github.com/yilinjz/astchunk)\n- [LEANN MCP Integration](../packages/leann-mcp/README.md)\n- [Research Paper](https://arxiv.org/html/2506.15655v1)\n\n---\n\n**Note**: AST chunking maintains full backward compatibility while enhancing code understanding capabilities.", "metadata": {}}
{"id": "44", "text": "\"\"\"\nComparison between Sentence Transformers and OpenAI embeddings\n\nThis example shows how different embedding models handle complex queries\nand demonstrates the differences between local and API-based embeddings.\n\"\"\"\n\nimport numpy as np\nfrom leann.embedding_compute import compute_embeddings\n\n# OpenAI API key should be set as environment variable\n# export OPENAI_API_KEY=\"your-api-key-here\"\n\n# Test data\nconference_text = \"[Title]: COLING 2025 Conference\\n[URL]: https://coling2025.org/\"\nbrowser_text = \"[Title]: Browser Use Tool\\n[URL]: https://github.com/browser-use\"\n\n# Two queries with same intent but different wording\nquery1 = \"Tell me my browser history about some conference i often visit\"\nquery2 = \"browser history about conference I often visit\"\n\ntexts = [query1, query2, conference_text, browser_text]\n\n\ndef cosine_similarity(a, b):\n    return np.dot(a, b)  # Already normalized\n\n\ndef analyze_embeddings(embeddings, model_name):\n    print(f\"\\n=== {model_name} Results ===\")", "metadata": {}}
{"id": "45", "text": "# Two queries with same intent but different wording\nquery1 = \"Tell me my browser history about some conference i often visit\"\nquery2 = \"browser history about conference I often visit\"\n\ntexts = [query1, query2, conference_text, browser_text]\n\n\ndef cosine_similarity(a, b):\n    return np.dot(a, b)  # Already normalized\n\n\ndef analyze_embeddings(embeddings, model_name):\n    print(f\"\\n=== {model_name} Results ===\")\n\n    # Results for Query 1\n    sim1_conf = cosine_similarity(embeddings[0], embeddings[2])\n    sim1_browser = cosine_similarity(embeddings[0], embeddings[3])", "metadata": {}}
{"id": "46", "text": "texts = [query1, query2, conference_text, browser_text]\n\n\ndef cosine_similarity(a, b):\n    return np.dot(a, b)  # Already normalized\n\n\ndef analyze_embeddings(embeddings, model_name):\n    print(f\"\\n=== {model_name} Results ===\")\n\n    # Results for Query 1\n    sim1_conf = cosine_similarity(embeddings[0], embeddings[2])\n    sim1_browser = cosine_similarity(embeddings[0], embeddings[3])\n\n    print(f\"Query 1: '{query1}'\")\n    print(f\"  ‚Üí Conference similarity: {sim1_conf:.4f} {'‚úì' if sim1_conf > sim1_browser else ''}\")\n    print(\n        f\"  ‚Üí Browser similarity:    {sim1_browser:.4f} {'‚úì' if sim1_browser > sim1_conf else ''}\"\n    )\n    print(f\"  Winner: {'Conference' if sim1_conf > sim1_browser else 'Browser'}\")", "metadata": {}}
{"id": "47", "text": "print(f\"Query 1: '{query1}'\")\n    print(f\"  ‚Üí Conference similarity: {sim1_conf:.4f} {'‚úì' if sim1_conf > sim1_browser else ''}\")\n    print(\n        f\"  ‚Üí Browser similarity:    {sim1_browser:.4f} {'‚úì' if sim1_browser > sim1_conf else ''}\"\n    )\n    print(f\"  Winner: {'Conference' if sim1_conf > sim1_browser else 'Browser'}\")\n\n    # Results for Query 2\n    sim2_conf = cosine_similarity(embeddings[1], embeddings[2])\n    sim2_browser = cosine_similarity(embeddings[1], embeddings[3])", "metadata": {}}
{"id": "48", "text": "# Results for Query 2\n    sim2_conf = cosine_similarity(embeddings[1], embeddings[2])\n    sim2_browser = cosine_similarity(embeddings[1], embeddings[3])\n\n    print(f\"\\nQuery 2: '{query2}'\")\n    print(f\"  ‚Üí Conference similarity: {sim2_conf:.4f} {'‚úì' if sim2_conf > sim2_browser else ''}\")\n    print(\n        f\"  ‚Üí Browser similarity:    {sim2_browser:.4f} {'‚úì' if sim2_browser > sim2_conf else ''}\"\n    )\n    print(f\"  Winner: {'Conference' if sim2_conf > sim2_browser else 'Browser'}\")\n\n    # Show the impact\n    print(\"\\n=== Impact Analysis ===\")\n    print(f\"Conference similarity change: {sim2_conf - sim1_conf:+.4f}\")\n    print(f\"Browser similarity change:    {sim2_browser - sim1_browser:+.4f}\")", "metadata": {}}
{"id": "49", "text": "# Show the impact\n    print(\"\\n=== Impact Analysis ===\")\n    print(f\"Conference similarity change: {sim2_conf - sim1_conf:+.4f}\")\n    print(f\"Browser similarity change:    {sim2_browser - sim1_browser:+.4f}\")\n\n    if sim1_conf > sim1_browser and sim2_browser > sim2_conf:\n        print(\"‚ùå FLIP: Adding 'browser history' flips winner from Conference to Browser!\")\n    elif sim1_conf > sim1_browser and sim2_conf > sim2_browser:\n        print(\"‚úÖ STABLE: Conference remains winner in both queries\")\n    elif sim1_browser > sim1_conf and sim2_browser > sim2_conf:\n        print(\"‚úÖ STABLE: Browser remains winner in both queries\")\n    else:\n        print(\"üîÑ MIXED: Results vary between queries\")\n\n    return {\n        \"query1_conf\": sim1_conf,\n        \"query1_browser\": sim1_browser,\n        \"query2_conf\": sim2_conf,\n        \"query2_browser\": sim2_browser,\n    }", "metadata": {}}
{"id": "50", "text": "return {\n        \"query1_conf\": sim1_conf,\n        \"query1_browser\": sim1_browser,\n        \"query2_conf\": sim2_conf,\n        \"query2_browser\": sim2_browser,\n    }\n\n\n# Test Sentence Transformers\nprint(\"Testing Sentence Transformers (facebook/contriever)...\")\ntry:\n    st_embeddings = compute_embeddings(texts, \"facebook/contriever\", mode=\"sentence-transformers\")\n    st_results = analyze_embeddings(st_embeddings, \"Sentence Transformers (facebook/contriever)\")\nexcept Exception as e:\n    print(f\"‚ùå Sentence Transformers failed: {e}\")\n    st_results = None\n\n# Test OpenAI\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Testing OpenAI (text-embedding-3-small)...\")\ntry:\n    openai_embeddings = compute_embeddings(texts, \"text-embedding-3-small\", mode=\"openai\")\n    openai_results = analyze_embeddings(openai_embeddings, \"OpenAI (text-embedding-3-small)\")\nexcept Exception as e:\n    print(f\"‚ùå OpenAI failed: {e}\")\n    openai_results = None", "metadata": {}}
{"id": "51", "text": "# Test OpenAI\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Testing OpenAI (text-embedding-3-small)...\")\ntry:\n    openai_embeddings = compute_embeddings(texts, \"text-embedding-3-small\", mode=\"openai\")\n    openai_results = analyze_embeddings(openai_embeddings, \"OpenAI (text-embedding-3-small)\")\nexcept Exception as e:\n    print(f\"‚ùå OpenAI failed: {e}\")\n    openai_results = None\n\n# Compare results\nif st_results and openai_results:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"=== COMPARISON SUMMARY ===\")", "metadata": {}}
{"id": "52", "text": "# LEANN Configuration Guide\n\nThis guide helps you optimize LEANN for different use cases and understand the trade-offs between various configuration options.\n\n## Getting Started: Simple is Better\n\nWhen first trying LEANN, start with a small dataset to quickly validate your approach:\n\n**For document RAG**: The default `data/` directory works perfectly - includes 2 AI research papers, Pride and Prejudice literature, and a technical report\n```bash\npython -m apps.document_rag --query \"What techniques does LEANN use?\"\n```\n\n**For other data sources**: Limit the dataset size for quick testing\n```bash\n# WeChat: Test with recent messages only\npython -m apps.wechat_rag --max-items 100 --query \"What did we discuss about the project timeline?\"\n\n# Browser history: Last few days\npython -m apps.browser_rag --max-items 500 --query \"Find documentation about vector databases\"\n\n# Email: Recent inbox\npython -m apps.email_rag --max-items 200 --query \"Who sent updates about the deployment status?\"\n```", "metadata": {}}
{"id": "53", "text": "**For other data sources**: Limit the dataset size for quick testing\n```bash\n# WeChat: Test with recent messages only\npython -m apps.wechat_rag --max-items 100 --query \"What did we discuss about the project timeline?\"\n\n# Browser history: Last few days\npython -m apps.browser_rag --max-items 500 --query \"Find documentation about vector databases\"\n\n# Email: Recent inbox\npython -m apps.email_rag --max-items 200 --query \"Who sent updates about the deployment status?\"\n```\n\nOnce validated, scale up gradually:\n- 100 documents ‚Üí 1,000 ‚Üí 10,000 ‚Üí full dataset (`--max-items -1`)\n- This helps identify issues early before committing to long processing times\n\n## Embedding Model Selection: Understanding the Trade-offs\n\nBased on our experience developing LEANN, embedding models fall into three categories:", "metadata": {}}
{"id": "54", "text": "# Email: Recent inbox\npython -m apps.email_rag --max-items 200 --query \"Who sent updates about the deployment status?\"\n```\n\nOnce validated, scale up gradually:\n- 100 documents ‚Üí 1,000 ‚Üí 10,000 ‚Üí full dataset (`--max-items -1`)\n- This helps identify issues early before committing to long processing times\n\n## Embedding Model Selection: Understanding the Trade-offs\n\nBased on our experience developing LEANN, embedding models fall into three categories:\n\n### Small Models (< 100M parameters)\n**Example**: `sentence-transformers/all-MiniLM-L6-v2` (22M params)\n- **Pros**: Lightweight, fast for both indexing and inference\n- **Cons**: Lower semantic understanding, may miss nuanced relationships\n- **Use when**: Speed is critical, handling simple queries, interactive mode, or just experimenting with LEANN. If time is not a constraint, consider using a larger/better embedding model", "metadata": {}}
{"id": "55", "text": "## Embedding Model Selection: Understanding the Trade-offs\n\nBased on our experience developing LEANN, embedding models fall into three categories:\n\n### Small Models (< 100M parameters)\n**Example**: `sentence-transformers/all-MiniLM-L6-v2` (22M params)\n- **Pros**: Lightweight, fast for both indexing and inference\n- **Cons**: Lower semantic understanding, may miss nuanced relationships\n- **Use when**: Speed is critical, handling simple queries, interactive mode, or just experimenting with LEANN. If time is not a constraint, consider using a larger/better embedding model\n\n### Medium Models (100M-500M parameters)\n**Example**: `facebook/contriever` (110M params), `BAAI/bge-base-en-v1.5` (110M params)\n- **Pros**: Balanced performance, good multilingual support, reasonable speed\n- **Cons**: Requires more compute than small models\n- **Use when**: Need quality results without extreme compute requirements, general-purpose RAG applications", "metadata": {}}
{"id": "56", "text": "### Medium Models (100M-500M parameters)\n**Example**: `facebook/contriever` (110M params), `BAAI/bge-base-en-v1.5` (110M params)\n- **Pros**: Balanced performance, good multilingual support, reasonable speed\n- **Cons**: Requires more compute than small models\n- **Use when**: Need quality results without extreme compute requirements, general-purpose RAG applications\n\n### Large Models (500M+ parameters)\n**Example**: `Qwen/Qwen3-Embedding-0.6B` (600M params), `intfloat/multilingual-e5-large` (560M params)\n- **Pros**: Best semantic understanding, captures complex relationships, excellent multilingual support. **Qwen3-Embedding-0.6B achieves nearly OpenAI API performance!**\n- **Cons**: Slower inference, longer index build times\n- **Use when**: Quality is paramount and you have sufficient compute resources. **Highly recommended** for production use\n\n### Quick Start: Cloud and Local Embedding Options", "metadata": {}}
{"id": "57", "text": "### Quick Start: Cloud and Local Embedding Options\n\n**OpenAI Embeddings (Fastest Setup)**\nFor immediate testing without local model downloads(also if you [do not have GPU](https://github.com/yichuan-w/LEANN/issues/43) and do not care that much about your document leak, you should use this, we compute the embedding and recompute using openai API):\n```bash\n# Set OpenAI embeddings (requires OPENAI_API_KEY)\n--embedding-mode openai --embedding-model text-embedding-3-small\n```\n\n**Ollama Embeddings (Privacy-Focused)**\nFor local embeddings with complete privacy:\n```bash\n# First, pull an embedding model\nollama pull nomic-embed-text\n\n# Use Ollama embeddings\n--embedding-mode ollama --embedding-model nomic-embed-text\n```\n\n<details>\n<summary><strong>Cloud vs Local Trade-offs</strong></summary>", "metadata": {}}
{"id": "58", "text": "**Ollama Embeddings (Privacy-Focused)**\nFor local embeddings with complete privacy:\n```bash\n# First, pull an embedding model\nollama pull nomic-embed-text\n\n# Use Ollama embeddings\n--embedding-mode ollama --embedding-model nomic-embed-text\n```\n\n<details>\n<summary><strong>Cloud vs Local Trade-offs</strong></summary>\n\n**OpenAI Embeddings** (`text-embedding-3-small/large`)\n- **Pros**: No local compute needed, consistently fast, high quality\n- **Cons**: Requires API key, costs money, data leaves your system, [known limitations with certain languages](https://yichuan-w.github.io/blog/lessons_learned_in_dev_leann/)\n- **When to use**: Prototyping, non-sensitive data, need immediate results\n\n**Local Embeddings**\n- **Pros**: Complete privacy, no ongoing costs, full control, can sometimes outperform OpenAI embeddings\n- **Cons**: Slower than cloud APIs, requires local compute resources\n- **When to use**: Production systems, sensitive data, cost-sensitive applications\n\n</details>", "metadata": {}}
{"id": "59", "text": "**Local Embeddings**\n- **Pros**: Complete privacy, no ongoing costs, full control, can sometimes outperform OpenAI embeddings\n- **Cons**: Slower than cloud APIs, requires local compute resources\n- **When to use**: Production systems, sensitive data, cost-sensitive applications\n\n</details>\n\n## Local & Remote Inference Endpoints\n\n> Applies to both LLMs (`leann ask`) and embeddings (`leann build`).\n\nLEANN now treats Ollama, LM Studio, and other OpenAI-compatible runtimes as first-class providers. You can point LEANN at any compatible endpoint ‚Äì either on the same machine or across the network ‚Äì with a couple of flags or environment variables.\n\n### One-Time Environment Setup\n\n```bash\n# Works for OpenAI-compatible runtimes such as LM Studio, vLLM, SGLang, llamafile, etc.\nexport OPENAI_API_KEY=\"your-key\"            # or leave unset for local servers that do not check keys\nexport OPENAI_BASE_URL=\"http://localhost:1234/v1\"", "metadata": {}}
{"id": "60", "text": "LEANN now treats Ollama, LM Studio, and other OpenAI-compatible runtimes as first-class providers. You can point LEANN at any compatible endpoint ‚Äì either on the same machine or across the network ‚Äì with a couple of flags or environment variables.\n\n### One-Time Environment Setup\n\n```bash\n# Works for OpenAI-compatible runtimes such as LM Studio, vLLM, SGLang, llamafile, etc.\nexport OPENAI_API_KEY=\"your-key\"            # or leave unset for local servers that do not check keys\nexport OPENAI_BASE_URL=\"http://localhost:1234/v1\"\n\n# Ollama-compatible runtimes (Ollama, Ollama on another host, llamacpp-server, etc.)\nexport LEANN_OLLAMA_HOST=\"http://localhost:11434\"   # falls back to OLLAMA_HOST or LOCAL_LLM_ENDPOINT\n```\n\nLEANN also recognises `LEANN_LOCAL_LLM_HOST` (highest priority), `LEANN_OPENAI_BASE_URL`, and `LOCAL_OPENAI_BASE_URL`, so existing scripts continue to work.\n\n### Passing Hosts Per Command", "metadata": {}}
{"id": "61", "text": "# Ollama-compatible runtimes (Ollama, Ollama on another host, llamacpp-server, etc.)\nexport LEANN_OLLAMA_HOST=\"http://localhost:11434\"   # falls back to OLLAMA_HOST or LOCAL_LLM_ENDPOINT\n```\n\nLEANN also recognises `LEANN_LOCAL_LLM_HOST` (highest priority), `LEANN_OPENAI_BASE_URL`, and `LOCAL_OPENAI_BASE_URL`, so existing scripts continue to work.\n\n### Passing Hosts Per Command\n\n```bash\n# Build an index with a remote embedding server\nleann build my-notes \\\n  --docs ./notes \\\n  --embedding-mode openai \\\n  --embedding-model text-embedding-qwen3-embedding-0.6b \\\n  --embedding-api-base http://192.168.1.50:1234/v1 \\\n  --embedding-api-key local-dev-key", "metadata": {}}
{"id": "62", "text": "LEANN also recognises `LEANN_LOCAL_LLM_HOST` (highest priority), `LEANN_OPENAI_BASE_URL`, and `LOCAL_OPENAI_BASE_URL`, so existing scripts continue to work.\n\n### Passing Hosts Per Command\n\n```bash\n# Build an index with a remote embedding server\nleann build my-notes \\\n  --docs ./notes \\\n  --embedding-mode openai \\\n  --embedding-model text-embedding-qwen3-embedding-0.6b \\\n  --embedding-api-base http://192.168.1.50:1234/v1 \\\n  --embedding-api-key local-dev-key\n\n# Query using a local LM Studio instance via OpenAI-compatible API\nleann ask my-notes \\\n  --llm openai \\\n  --llm-model qwen3-8b \\\n  --api-base http://localhost:1234/v1 \\\n  --api-key local-dev-key", "metadata": {}}
{"id": "63", "text": "# Query using a local LM Studio instance via OpenAI-compatible API\nleann ask my-notes \\\n  --llm openai \\\n  --llm-model qwen3-8b \\\n  --api-base http://localhost:1234/v1 \\\n  --api-key local-dev-key\n\n# Query an Ollama instance running on another box\nleann ask my-notes \\\n  --llm ollama \\\n  --llm-model qwen3:14b \\\n  --host http://192.168.1.101:11434\n```\n\n‚ö†Ô∏è **Make sure the endpoint is reachable**: when your inference server runs on a home/workstation and the index/search job runs in the cloud, the server must be able to reach the host you configured. Typical options include:\n\n- Expose a public IP (and open the relevant port) on the machine that hosts LM Studio/Ollama.\n- Configure router or cloud provider port forwarding.\n- Tunnel traffic through tools like `tailscale`, `cloudflared`, or `ssh -R`.", "metadata": {}}
{"id": "64", "text": "‚ö†Ô∏è **Make sure the endpoint is reachable**: when your inference server runs on a home/workstation and the index/search job runs in the cloud, the server must be able to reach the host you configured. Typical options include:\n\n- Expose a public IP (and open the relevant port) on the machine that hosts LM Studio/Ollama.\n- Configure router or cloud provider port forwarding.\n- Tunnel traffic through tools like `tailscale`, `cloudflared`, or `ssh -R`.\n\nWhen you set these options while building an index, LEANN stores them in `meta.json`. Any subsequent `leann ask` or searcher process automatically reuses the same provider settings ‚Äì even when we spawn background embedding servers. This makes the ‚Äúserver without GPU talking to my local workstation‚Äù workflow from [issue #80](https://github.com/yichuan-w/LEANN/issues/80#issuecomment-2287230548) work out-of-the-box.\n\n**Tip:** If your runtime does not require an API key (many local stacks don‚Äôt), leave `--api-key` unset. LEANN will skip injecting credentials.", "metadata": {}}
{"id": "65", "text": "**Tip:** If your runtime does not require an API key (many local stacks don‚Äôt), leave `--api-key` unset. LEANN will skip injecting credentials.\n\n### Python API Usage\n\nYou can pass the same configuration from Python:\n\n```python\nfrom leann.api import LeannBuilder\n\nbuilder = LeannBuilder(\n    backend_name=\"hnsw\",\n    embedding_mode=\"openai\",\n    embedding_model=\"text-embedding-qwen3-embedding-0.6b\",\n    embedding_options={\n        \"base_url\": \"http://192.168.1.50:1234/v1\",\n        \"api_key\": \"local-dev-key\",\n    },\n)\nbuilder.build_index(\"./indexes/my-notes\", chunks)\n```\n\n`embedding_options` is persisted to the index `meta.json`, so subsequent `LeannSearcher` or `LeannChat` sessions automatically reuse the same provider settings (the embedding server manager forwards them to the provider for you).\n\n## Optional Embedding Features\n\n### Task-Specific Prompt Templates", "metadata": {}}
{"id": "66", "text": "`embedding_options` is persisted to the index `meta.json`, so subsequent `LeannSearcher` or `LeannChat` sessions automatically reuse the same provider settings (the embedding server manager forwards them to the provider for you).\n\n## Optional Embedding Features\n\n### Task-Specific Prompt Templates\n\nSome embedding models are trained with task-specific prompts to differentiate between documents and queries. The most notable example is **Google's EmbeddingGemma**, which requires different prompts depending on the use case:\n\n- **Indexing documents**: `\"title: none | text: \"`\n- **Search queries**: `\"task: search result | query: \"`\n\nLEANN supports automatic prompt prepending via the `--embedding-prompt-template` flag:", "metadata": {}}
{"id": "67", "text": "## Optional Embedding Features\n\n### Task-Specific Prompt Templates\n\nSome embedding models are trained with task-specific prompts to differentiate between documents and queries. The most notable example is **Google's EmbeddingGemma**, which requires different prompts depending on the use case:\n\n- **Indexing documents**: `\"title: none | text: \"`\n- **Search queries**: `\"task: search result | query: \"`\n\nLEANN supports automatic prompt prepending via the `--embedding-prompt-template` flag:\n\n```bash\n# Build index with EmbeddingGemma (via LM Studio or Ollama)\nleann build my-docs \\\n  --docs ./documents \\\n  --embedding-mode openai \\\n  --embedding-model text-embedding-embeddinggemma-300m-qat \\\n  --embedding-api-base http://localhost:1234/v1 \\\n  --embedding-prompt-template \"title: none | text: \" \\\n  --force\n\n# Search with query-specific prompt\nleann search my-docs \\\n  --query \"What is quantum computing?\" \\\n  --embedding-prompt-template \"task: search result | query: \"\n```", "metadata": {}}
{"id": "68", "text": "# Search with query-specific prompt\nleann search my-docs \\\n  --query \"What is quantum computing?\" \\\n  --embedding-prompt-template \"task: search result | query: \"\n```\n\nA full example that is used for building the LEANN's repo during dev:\n```\nsource \"$LEANN_PATH/.venv/bin/activate\" && \\\nleann build --docs $(git ls-files | grep -Ev '\\.(png|jpg|jpeg|gif|yml|yaml|sh|pdf|JPG)$') --embedding-mode openai \\\n--embedding-model text-embedding-embeddinggemma-300m-qat \\\n--embedding-prompt-template \"title: none | text: \" \\\n--query-prompt-template \"task: search result | query: \" \\\n--embedding-api-key local-dev-key \\\n--embedding-api-base http://localhost:1234/v1 \\\n--doc-chunk-size 1024 --doc-chunk-overlap 100 \\\n--code-chunk-size 1024 --code-chunk-overlap 100 \\\n--ast-chunk-size 1024 --ast-chunk-overlap 100 \\\n--force --use-ast-chunking --no-compact --no-recompute\n```", "metadata": {}}
{"id": "69", "text": "**Important Notes:**\n- **Only use with compatible models**: EmbeddingGemma and similar task-specific models\n- **NOT for regular models**: Adding prompts to models like `nomic-embed-text`, `text-embedding-3-small`, or `bge-base-en-v1.5` will corrupt embeddings\n- **Template is saved**: Build-time templates are saved to `.meta.json` for reference; you can add both `--embedding-prompt-template` and `--query-prompt-template` values during building phase, and this way the mcp query will automatically pick up the query template\n- **Flexible prompts**: You can use any prompt string, or leave it empty (`\"\"`)\n\n**Python API:**\n```python\nfrom leann.api import LeannBuilder", "metadata": {}}
{"id": "70", "text": "**Python API:**\n```python\nfrom leann.api import LeannBuilder\n\nbuilder = LeannBuilder(\n    embedding_mode=\"openai\",\n    embedding_model=\"text-embedding-embeddinggemma-300m-qat\",\n    embedding_options={\n        \"base_url\": \"http://localhost:1234/v1\",\n        \"api_key\": \"lm-studio\",\n        \"prompt_template\": \"title: none | text: \",\n    },\n)\nbuilder.build_index(\"./indexes/my-docs\", chunks)\n```\n\n**References:**\n- [HuggingFace Blog: EmbeddingGemma](https://huggingface.co/blog/embeddinggemma) - Technical details\n\n### LM Studio Auto-Detection (Optional)\n\nWhen using LM Studio with the OpenAI-compatible API, LEANN can optionally auto-detect model context lengths via the LM Studio SDK. This eliminates manual configuration for token limits.\n\n**Prerequisites:**\n```bash\n# Install Node.js (if not already installed)\n# Then install the LM Studio SDK globally\nnpm install -g @lmstudio/sdk\n```", "metadata": {}}
{"id": "71", "text": "**References:**\n- [HuggingFace Blog: EmbeddingGemma](https://huggingface.co/blog/embeddinggemma) - Technical details\n\n### LM Studio Auto-Detection (Optional)\n\nWhen using LM Studio with the OpenAI-compatible API, LEANN can optionally auto-detect model context lengths via the LM Studio SDK. This eliminates manual configuration for token limits.\n\n**Prerequisites:**\n```bash\n# Install Node.js (if not already installed)\n# Then install the LM Studio SDK globally\nnpm install -g @lmstudio/sdk\n```\n\n**How it works:**\n1. LEANN detects LM Studio URLs (`:1234`, `lmstudio` in URL)\n2. Queries model metadata via Node.js subprocess\n3. Automatically unloads model after query (respects your JIT auto-evict settings)\n4. Falls back to static registry if SDK unavailable\n\n**No configuration needed** - it works automatically when SDK is installed:", "metadata": {}}
{"id": "72", "text": "**Prerequisites:**\n```bash\n# Install Node.js (if not already installed)\n# Then install the LM Studio SDK globally\nnpm install -g @lmstudio/sdk\n```\n\n**How it works:**\n1. LEANN detects LM Studio URLs (`:1234`, `lmstudio` in URL)\n2. Queries model metadata via Node.js subprocess\n3. Automatically unloads model after query (respects your JIT auto-evict settings)\n4. Falls back to static registry if SDK unavailable\n\n**No configuration needed** - it works automatically when SDK is installed:\n\n```bash\nleann build my-docs \\\n  --docs ./documents \\\n  --embedding-mode openai \\\n  --embedding-model text-embedding-nomic-embed-text-v1.5 \\\n  --embedding-api-base http://localhost:1234/v1\n  # Context length auto-detected if SDK available\n  # Falls back to registry (2048) if not\n```", "metadata": {}}
{"id": "73", "text": "**No configuration needed** - it works automatically when SDK is installed:\n\n```bash\nleann build my-docs \\\n  --docs ./documents \\\n  --embedding-mode openai \\\n  --embedding-model text-embedding-nomic-embed-text-v1.5 \\\n  --embedding-api-base http://localhost:1234/v1\n  # Context length auto-detected if SDK available\n  # Falls back to registry (2048) if not\n```\n\n**Benefits:**\n- ‚úÖ Automatic token limit detection\n- ‚úÖ Respects LM Studio JIT auto-evict settings\n- ‚úÖ No manual registry maintenance\n- ‚úÖ Graceful fallback if SDK unavailable\n\n**Note:** This is completely optional. LEANN works perfectly fine without the SDK using the built-in token limit registry.\n\n## Index Selection: Matching Your Scale\n\n### HNSW (Hierarchical Navigable Small World)\n**Best for**: Small to medium datasets (< 10M vectors) - **Default and recommended for extreme low storage**\n- Full recomputation required\n- High memory usage during build phase\n- Excellent recall (95%+)", "metadata": {}}
{"id": "74", "text": "**Note:** This is completely optional. LEANN works perfectly fine without the SDK using the built-in token limit registry.\n\n## Index Selection: Matching Your Scale\n\n### HNSW (Hierarchical Navigable Small World)\n**Best for**: Small to medium datasets (< 10M vectors) - **Default and recommended for extreme low storage**\n- Full recomputation required\n- High memory usage during build phase\n- Excellent recall (95%+)\n\n```bash\n# Optimal for most use cases\n--backend-name hnsw --graph-degree 32 --build-complexity 64\n```\n\n### DiskANN\n**Best for**: Large datasets, especially when you want `recompute=True`.\n\n**Key advantages:**\n- **Faster search** on large datasets (3x+ speedup vs HNSW in many cases)\n- **Smart storage**: `recompute=True` enables automatic graph partitioning for smaller indexes\n- **Better scaling**: Designed for 100k+ documents", "metadata": {}}
{"id": "75", "text": "```bash\n# Optimal for most use cases\n--backend-name hnsw --graph-degree 32 --build-complexity 64\n```\n\n### DiskANN\n**Best for**: Large datasets, especially when you want `recompute=True`.\n\n**Key advantages:**\n- **Faster search** on large datasets (3x+ speedup vs HNSW in many cases)\n- **Smart storage**: `recompute=True` enables automatic graph partitioning for smaller indexes\n- **Better scaling**: Designed for 100k+ documents\n\n**Recompute behavior:**\n- `recompute=True` (recommended): Pure PQ traversal + final reranking - faster and enables partitioning\n- `recompute=False`: PQ + partial real distances during traversal - slower but higher accuracy\n\n```bash\n# Recommended for most use cases\n--backend-name diskann --graph-degree 32 --build-complexity 64\n```\n\n**Performance Benchmark**: Run `uv run benchmarks/diskann_vs_hnsw_speed_comparison.py` to compare DiskANN and HNSW on your system.\n\n## LLM Selection: Engine and Model Comparison", "metadata": {}}
{"id": "76", "text": "**Recompute behavior:**\n- `recompute=True` (recommended): Pure PQ traversal + final reranking - faster and enables partitioning\n- `recompute=False`: PQ + partial real distances during traversal - slower but higher accuracy\n\n```bash\n# Recommended for most use cases\n--backend-name diskann --graph-degree 32 --build-complexity 64\n```\n\n**Performance Benchmark**: Run `uv run benchmarks/diskann_vs_hnsw_speed_comparison.py` to compare DiskANN and HNSW on your system.\n\n## LLM Selection: Engine and Model Comparison\n\n### LLM Engines", "metadata": {}}
{"id": "77", "text": "**Recompute behavior:**\n- `recompute=True` (recommended): Pure PQ traversal + final reranking - faster and enables partitioning\n- `recompute=False`: PQ + partial real distances during traversal - slower but higher accuracy\n\n```bash\n# Recommended for most use cases\n--backend-name diskann --graph-degree 32 --build-complexity 64\n```\n\n**Performance Benchmark**: Run `uv run benchmarks/diskann_vs_hnsw_speed_comparison.py` to compare DiskANN and HNSW on your system.\n\n## LLM Selection: Engine and Model Comparison\n\n### LLM Engines\n\n**OpenAI** (`--llm openai`)\n- **Pros**: Best quality, consistent performance, no local resources needed\n- **Cons**: Costs money ($0.15-2.5 per million tokens), requires internet, data privacy concerns\n- **Models**: `gpt-4o-mini` (fast, cheap), `gpt-4o` (best quality), `o3` (reasoning), `o3-mini` (reasoning, cheaper)\n- **Thinking Budget**: Use `--thinking-budget low/medium/high` for o-series reasoning models (o3, o3-mini, o4-mini)\n- **Note**: Our current default, but we recommend switching to Ollama for most use cases", "metadata": {}}
{"id": "78", "text": "**Ollama** (`--llm ollama`)\n- **Pros**: Fully local, free, privacy-preserving, good model variety\n- **Cons**: Requires local GPU/CPU resources, slower than cloud APIs, need to install extra [ollama app](https://github.com/ollama/ollama?tab=readme-ov-file#ollama) and pre-download models by `ollama pull`\n- **Models**: `qwen3:0.6b` (ultra-fast), `qwen3:1.7b` (balanced), `qwen3:4b` (good quality), `qwen3:7b` (high quality), `deepseek-r1:1.5b` (reasoning)\n- **Thinking Budget**: Use `--thinking-budget low/medium/high` for reasoning models like GPT-Oss:20b", "metadata": {}}
{"id": "79", "text": "**HuggingFace** (`--llm hf`)\n- **Pros**: Free tier available, huge model selection, direct model loading (vs Ollama's server-based approach)\n- **Cons**: More complex initial setup\n- **Models**: `Qwen/Qwen3-1.7B-FP8`\n\n## Parameter Tuning Guide\n\n### Search Complexity Parameters\n\n**`--build-complexity`** (index building)\n- Controls thoroughness during index construction\n- Higher = better recall but slower build\n- Recommendations:\n  - 32: Quick prototyping\n  - 64: Balanced (default)\n  - 128: Production systems\n  - 256: Maximum quality\n\n**`--search-complexity`** (query time)\n- Controls search thoroughness\n- Higher = better results but slower\n- Recommendations:\n  - 16: Fast/Interactive search\n  - 32: High quality with diversity\n  - 64+: Maximum accuracy\n\n### Top-K Selection", "metadata": {}}
{"id": "80", "text": "**`--search-complexity`** (query time)\n- Controls search thoroughness\n- Higher = better results but slower\n- Recommendations:\n  - 16: Fast/Interactive search\n  - 32: High quality with diversity\n  - 64+: Maximum accuracy\n\n### Top-K Selection\n\n**`--top-k`** (number of retrieved chunks)\n- More chunks = better context but slower LLM processing\n- Should be always smaller than `--search-complexity`\n- Guidelines:\n  - 10-20: General questions (default: 20)\n  - 30+: Complex multi-hop reasoning requiring comprehensive context\n\n**Trade-off formula**:\n- Retrieval time ‚àù log(n) √ó search_complexity\n- LLM processing time ‚àù top_k √ó chunk_size\n- Total context = top_k √ó chunk_size tokens\n\n### Thinking Budget for Reasoning Models", "metadata": {}}
{"id": "81", "text": "### Top-K Selection\n\n**`--top-k`** (number of retrieved chunks)\n- More chunks = better context but slower LLM processing\n- Should be always smaller than `--search-complexity`\n- Guidelines:\n  - 10-20: General questions (default: 20)\n  - 30+: Complex multi-hop reasoning requiring comprehensive context\n\n**Trade-off formula**:\n- Retrieval time ‚àù log(n) √ó search_complexity\n- LLM processing time ‚àù top_k √ó chunk_size\n- Total context = top_k √ó chunk_size tokens\n\n### Thinking Budget for Reasoning Models\n\n**`--thinking-budget`** (reasoning effort level)\n- Controls the computational effort for reasoning models\n- Options: `low`, `medium`, `high`\n- Guidelines:\n  - `low`: Fast responses, basic reasoning (default for simple queries)\n  - `medium`: Balanced speed and reasoning depth\n  - `high`: Maximum reasoning effort, best for complex analytical questions\n- **Supported Models**:\n  - **Ollama**: `gpt-oss:20b`, `gpt-oss:120b`\n  - **OpenAI**: `o3`, `o3-mini`, `o4-mini`, `o1` (o-series reasoning models)\n- **Note**: Models without reasoning support will show a warning and proceed without reasoning parameters\n- **Example**: `--thinking-budget high` for complex analytical questions", "metadata": {}}
{"id": "82", "text": "**üìñ For detailed usage examples and implementation details, check out [Thinking Budget Documentation](THINKING_BUDGET_FEATURE.md)**\n\n**üí° Quick Examples:**\n```bash\n# OpenAI o-series reasoning model\npython apps/document_rag.py --query \"What are the main techniques LEANN explores?\" \\\n  --index-dir hnswbuild --backend hnsw \\\n  --llm openai --llm-model o3 --thinking-budget medium\n\n# Ollama reasoning model\npython apps/document_rag.py --query \"What are the main techniques LEANN explores?\" \\\n  --index-dir hnswbuild --backend hnsw \\\n  --llm ollama --llm-model gpt-oss:20b --thinking-budget high\n```\n\n### Graph Degree (HNSW/DiskANN)\n\n**`--graph-degree`**\n- Number of connections per node in the graph\n- Higher = better recall but more memory\n- HNSW: 16-32 (default: 32)\n- DiskANN: 32-128 (default: 64)\n\n\n## Performance Optimization Checklist", "metadata": {}}
{"id": "83", "text": "### Graph Degree (HNSW/DiskANN)\n\n**`--graph-degree`**\n- Number of connections per node in the graph\n- Higher = better recall but more memory\n- HNSW: 16-32 (default: 32)\n- DiskANN: 32-128 (default: 64)\n\n\n## Performance Optimization Checklist\n\n### If Embedding is Too Slow\n\n1. **Switch to smaller model**:\n   ```bash\n   # From large model\n   --embedding-model Qwen/Qwen3-Embedding-0.6B\n   # To small model\n   --embedding-model sentence-transformers/all-MiniLM-L6-v2\n   ```\n\n2. **Limit dataset size for testing**:\n   ```bash\n   --max-items 1000  # Process first 1k items only\n   ```", "metadata": {}}
{"id": "84", "text": "## Performance Optimization Checklist\n\n### If Embedding is Too Slow\n\n1. **Switch to smaller model**:\n   ```bash\n   # From large model\n   --embedding-model Qwen/Qwen3-Embedding-0.6B\n   # To small model\n   --embedding-model sentence-transformers/all-MiniLM-L6-v2\n   ```\n\n2. **Limit dataset size for testing**:\n   ```bash\n   --max-items 1000  # Process first 1k items only\n   ```\n\n3. **Use MLX on Apple Silicon** (optional optimization):\n   ```bash\n   --embedding-mode mlx --embedding-model mlx-community/Qwen3-Embedding-0.6B-8bit\n   ```\n    MLX might not be the best choice, as we tested and found that it only offers 1.3x acceleration compared to HF, so maybe using ollama is a better choice for embedding generation", "metadata": {}}
{"id": "85", "text": "2. **Limit dataset size for testing**:\n   ```bash\n   --max-items 1000  # Process first 1k items only\n   ```\n\n3. **Use MLX on Apple Silicon** (optional optimization):\n   ```bash\n   --embedding-mode mlx --embedding-model mlx-community/Qwen3-Embedding-0.6B-8bit\n   ```\n    MLX might not be the best choice, as we tested and found that it only offers 1.3x acceleration compared to HF, so maybe using ollama is a better choice for embedding generation\n\n4. **Use Ollama**\n   ```bash\n   --embedding-mode ollama --embedding-model nomic-embed-text\n   ```\n   To discover additional embedding models in ollama, check out https://ollama.com/search?c=embedding or read more about embedding models at https://ollama.com/blog/embedding-models, please do check the model size that works best for you\n### If Search Quality is Poor", "metadata": {}}
{"id": "86", "text": "4. **Use Ollama**\n   ```bash\n   --embedding-mode ollama --embedding-model nomic-embed-text\n   ```\n   To discover additional embedding models in ollama, check out https://ollama.com/search?c=embedding or read more about embedding models at https://ollama.com/blog/embedding-models, please do check the model size that works best for you\n### If Search Quality is Poor\n\n1. **Increase retrieval count**:\n   ```bash\n   --top-k 30  # Retrieve more candidates\n   ```\n\n2. **Upgrade embedding model**:\n   ```bash\n   # For English\n   --embedding-model BAAI/bge-base-en-v1.5\n   # For multilingual\n   --embedding-model intfloat/multilingual-e5-large\n   ```\n\n## Understanding the Trade-offs\n\nEvery configuration choice involves trade-offs:", "metadata": {}}
{"id": "87", "text": "1. **Increase retrieval count**:\n   ```bash\n   --top-k 30  # Retrieve more candidates\n   ```\n\n2. **Upgrade embedding model**:\n   ```bash\n   # For English\n   --embedding-model BAAI/bge-base-en-v1.5\n   # For multilingual\n   --embedding-model intfloat/multilingual-e5-large\n   ```\n\n## Understanding the Trade-offs\n\nEvery configuration choice involves trade-offs:\n\n| Factor | Small/Fast | Large/Quality |\n|--------|------------|---------------|\n| Embedding Model | `all-MiniLM-L6-v2` | `Qwen/Qwen3-Embedding-0.6B` |\n| Chunk Size | 512 tokens | 128 tokens |\n| Index Type | HNSW | DiskANN |\n| LLM | `qwen3:1.7b` | `gpt-4o` |\n\nThe key is finding the right balance for your specific use case. Start small and simple, measure performance, then scale up only where needed.\n\n## Low-resource setups", "metadata": {}}
{"id": "88", "text": "The key is finding the right balance for your specific use case. Start small and simple, measure performance, then scale up only where needed.\n\n## Low-resource setups\n\nIf you don‚Äôt have a local GPU or builds/searches are too slow, use one or more of the options below.\n\n### 1) Use OpenAI embeddings (no local compute)\n\nFastest path with zero local GPU requirements. Set your API key and use OpenAI embeddings during build and search:\n\n```bash\nexport OPENAI_API_KEY=sk-...\n\n# Build with OpenAI embeddings\nleann build my-index \\\n  --embedding-mode openai \\\n  --embedding-model text-embedding-3-small\n\n# Search with OpenAI embeddings (recompute at query time)\nleann search my-index \"your query\" \\\n  --recompute\n```\n\n### 2) Run remote builds with SkyPilot (cloud GPU)\n\nOffload embedding generation and index building to a GPU VM using [SkyPilot](https://docs.skypilot.co/en/latest/docs/index.html). A template is provided at `sky/leann-build.yaml`.", "metadata": {}}
{"id": "89", "text": "# Build with OpenAI embeddings\nleann build my-index \\\n  --embedding-mode openai \\\n  --embedding-model text-embedding-3-small\n\n# Search with OpenAI embeddings (recompute at query time)\nleann search my-index \"your query\" \\\n  --recompute\n```\n\n### 2) Run remote builds with SkyPilot (cloud GPU)\n\nOffload embedding generation and index building to a GPU VM using [SkyPilot](https://docs.skypilot.co/en/latest/docs/index.html). A template is provided at `sky/leann-build.yaml`.\n\n```bash\n# One-time: install and configure SkyPilot\npip install skypilot\n\n# Launch with defaults (L4:1) and mount ./data to ~/leann-data; the build runs automatically\nsky launch -c leann-gpu sky/leann-build.yaml", "metadata": {}}
{"id": "90", "text": "### 2) Run remote builds with SkyPilot (cloud GPU)\n\nOffload embedding generation and index building to a GPU VM using [SkyPilot](https://docs.skypilot.co/en/latest/docs/index.html). A template is provided at `sky/leann-build.yaml`.\n\n```bash\n# One-time: install and configure SkyPilot\npip install skypilot\n\n# Launch with defaults (L4:1) and mount ./data to ~/leann-data; the build runs automatically\nsky launch -c leann-gpu sky/leann-build.yaml\n\n# Override parameters via -e key=value (optional)\nsky launch -c leann-gpu sky/leann-build.yaml \\\n  -e index_name=my-index \\\n  -e backend=hnsw \\\n  -e embedding_mode=sentence-transformers \\\n  -e embedding_model=Qwen/Qwen3-Embedding-0.6B", "metadata": {}}
{"id": "91", "text": "# Launch with defaults (L4:1) and mount ./data to ~/leann-data; the build runs automatically\nsky launch -c leann-gpu sky/leann-build.yaml\n\n# Override parameters via -e key=value (optional)\nsky launch -c leann-gpu sky/leann-build.yaml \\\n  -e index_name=my-index \\\n  -e backend=hnsw \\\n  -e embedding_mode=sentence-transformers \\\n  -e embedding_model=Qwen/Qwen3-Embedding-0.6B\n\n# Copy the built index back to your local .leann (use rsync)\nrsync -Pavz leann-gpu:~/.leann/indexes/my-index ./.leann/indexes/\n```\n\n### 3) Disable recomputation to trade storage for speed\n\nIf you need lower latency and have more storage/memory, disable recomputation. This stores full embeddings and avoids recomputing at search time.\n\n```bash\n# Build without recomputation (HNSW requires non-compact in this mode)\nleann build my-index --no-recompute --no-compact", "metadata": {}}
{"id": "92", "text": "# Copy the built index back to your local .leann (use rsync)\nrsync -Pavz leann-gpu:~/.leann/indexes/my-index ./.leann/indexes/\n```\n\n### 3) Disable recomputation to trade storage for speed\n\nIf you need lower latency and have more storage/memory, disable recomputation. This stores full embeddings and avoids recomputing at search time.\n\n```bash\n# Build without recomputation (HNSW requires non-compact in this mode)\nleann build my-index --no-recompute --no-compact\n\n# Search without recomputation\nleann search my-index \"your query\" --no-recompute\n```\n\nWhen to use:\n- Extreme low latency requirements (high QPS, interactive assistants)\n- Read-heavy workloads where storage is cheaper than latency\n- No always-available GPU\n\nConstraints:\n- HNSW: when `--no-recompute` is set, LEANN automatically disables compact mode during build\n- DiskANN: supported; `--no-recompute` skips selective recompute during search", "metadata": {}}
{"id": "93", "text": "# Search without recomputation\nleann search my-index \"your query\" --no-recompute\n```\n\nWhen to use:\n- Extreme low latency requirements (high QPS, interactive assistants)\n- Read-heavy workloads where storage is cheaper than latency\n- No always-available GPU\n\nConstraints:\n- HNSW: when `--no-recompute` is set, LEANN automatically disables compact mode during build\n- DiskANN: supported; `--no-recompute` skips selective recompute during search\n\nStorage impact:\n- Storing N embeddings of dimension D with float32 requires approximately N √ó D √ó 4 bytes\n- Example: 1,000,000 chunks √ó 768 dims √ó 4 bytes ‚âà 2.86 GB (plus graph/metadata)\n\nConverting an existing index (rebuild required):\n```bash\n# Rebuild in-place (ensure you still have original docs or can regenerate chunks)\nleann build my-index --force --no-recompute --no-compact\n```\n\nPython API usage:\n```python\nfrom leann import LeannSearcher", "metadata": {}}
{"id": "94", "text": "Storage impact:\n- Storing N embeddings of dimension D with float32 requires approximately N √ó D √ó 4 bytes\n- Example: 1,000,000 chunks √ó 768 dims √ó 4 bytes ‚âà 2.86 GB (plus graph/metadata)\n\nConverting an existing index (rebuild required):\n```bash\n# Rebuild in-place (ensure you still have original docs or can regenerate chunks)\nleann build my-index --force --no-recompute --no-compact\n```\n\nPython API usage:\n```python\nfrom leann import LeannSearcher\n\nsearcher = LeannSearcher(\"/path/to/my-index.leann\")\nresults = searcher.search(\"your query\", top_k=10, recompute_embeddings=False)\n```\n\nTrade-offs:\n- Lower latency and fewer network hops at query time\n- Significantly higher storage (10‚Äì100√ó vs selective recomputation)\n- Slightly larger memory footprint during build and search\n\nQuick benchmark results (`benchmarks/benchmark_no_recompute.py` with 5k texts, complexity=32):\n\n- HNSW", "metadata": {}}
{"id": "95", "text": "Python API usage:\n```python\nfrom leann import LeannSearcher\n\nsearcher = LeannSearcher(\"/path/to/my-index.leann\")\nresults = searcher.search(\"your query\", top_k=10, recompute_embeddings=False)\n```\n\nTrade-offs:\n- Lower latency and fewer network hops at query time\n- Significantly higher storage (10‚Äì100√ó vs selective recomputation)\n- Slightly larger memory footprint during build and search\n\nQuick benchmark results (`benchmarks/benchmark_no_recompute.py` with 5k texts, complexity=32):\n\n- HNSW\n\n  ```text\n  recompute=True:  search_time=0.818s, size=1.1MB\n  recompute=False: search_time=0.012s, size=16.6MB\n  ```\n\n- DiskANN\n\n  ```text\n  recompute=True:  search_time=0.041s, size=5.9MB\n  recompute=False: search_time=0.013s, size=24.6MB\n  ```", "metadata": {}}
{"id": "96", "text": "- HNSW\n\n  ```text\n  recompute=True:  search_time=0.818s, size=1.1MB\n  recompute=False: search_time=0.012s, size=16.6MB\n  ```\n\n- DiskANN\n\n  ```text\n  recompute=True:  search_time=0.041s, size=5.9MB\n  recompute=False: search_time=0.013s, size=24.6MB\n  ```\n\nConclusion:\n- **HNSW**: `no-recompute` is significantly faster (no embedding recomputation) but requires much more storage (stores all embeddings)\n- **DiskANN**: `no-recompute` uses PQ + partial real distances during traversal (slower but higher accuracy), while `recompute=True` uses pure PQ traversal + final reranking (faster traversal, enables build-time partitioning for smaller storage)\n\n\n\n## Further Reading", "metadata": {}}
{"id": "97", "text": "Conclusion:\n- **HNSW**: `no-recompute` is significantly faster (no embedding recomputation) but requires much more storage (stores all embeddings)\n- **DiskANN**: `no-recompute` uses PQ + partial real distances during traversal (slower but higher accuracy), while `recompute=True` uses pure PQ traversal + final reranking (faster traversal, enables build-time partitioning for smaller storage)\n\n\n\n## Further Reading\n\n- [Lessons Learned Developing LEANN](https://yichuan-w.github.io/blog/lessons_learned_in_dev_leann/)\n- [LEANN Technical Paper](https://arxiv.org/abs/2506.08276)\n- [DiskANN Original Paper](https://suhasjs.github.io/files/diskann_neurips19.pdf)\n- [SSD-based Graph Partitioning](https://github.com/SonglinLife/SSD_BASED_PLAN)", "metadata": {}}
{"id": "98", "text": "# FAQ\n\n## 1. My building time seems long\n\nYou can speed up the process by using a lightweight embedding model. Add this to your arguments:\n\n```bash\n--embedding-model sentence-transformers/all-MiniLM-L6-v2\n```\n**Model sizes:** `all-MiniLM-L6-v2` (30M parameters), `facebook/contriever` (~100M parameters), `Qwen3-0.6B` (600M parameters)\n\n## 2. When should I use prompt templates?\n\n**Use prompt templates ONLY with task-specific embedding models** like Google's EmbeddingGemma. These models are specially trained to use different prompts for documents vs queries.\n\n**DO NOT use with regular models** like `nomic-embed-text`, `text-embedding-3-small`, or `bge-base-en-v1.5` - adding prompts to these models will corrupt the embeddings.\n\n**Example usage with EmbeddingGemma:**\n```bash\n# Build with document prompt\nleann build my-docs --embedding-prompt-template \"title: none | text: \"", "metadata": {}}
{"id": "99", "text": "**Use prompt templates ONLY with task-specific embedding models** like Google's EmbeddingGemma. These models are specially trained to use different prompts for documents vs queries.\n\n**DO NOT use with regular models** like `nomic-embed-text`, `text-embedding-3-small`, or `bge-base-en-v1.5` - adding prompts to these models will corrupt the embeddings.\n\n**Example usage with EmbeddingGemma:**\n```bash\n# Build with document prompt\nleann build my-docs --embedding-prompt-template \"title: none | text: \"\n\n# Search with query prompt\nleann search my-docs --query \"your question\" --embedding-prompt-template \"task: search result | query: \"\n```\n\nSee the [Configuration Guide: Task-Specific Prompt Templates](configuration-guide.md#task-specific-prompt-templates) for detailed usage.\n\n## 3. Why is LM Studio loading multiple copies of my model?\n\nThis was fixed in recent versions. LEANN now properly unloads models after querying metadata, respecting your LM Studio JIT auto-evict settings.", "metadata": {}}
{"id": "100", "text": "# Search with query prompt\nleann search my-docs --query \"your question\" --embedding-prompt-template \"task: search result | query: \"\n```\n\nSee the [Configuration Guide: Task-Specific Prompt Templates](configuration-guide.md#task-specific-prompt-templates) for detailed usage.\n\n## 3. Why is LM Studio loading multiple copies of my model?\n\nThis was fixed in recent versions. LEANN now properly unloads models after querying metadata, respecting your LM Studio JIT auto-evict settings.\n\n**If you still see duplicates:**\n- Update to the latest LEANN version\n- Restart LM Studio to clear loaded models\n- Check that you have JIT auto-evict enabled in LM Studio settings\n\n**How it works now:**\n1. LEANN loads model temporarily to get context length\n2. Immediately unloads after query\n3. LM Studio JIT loads model on-demand for actual embeddings\n4. Auto-evicts per your settings\n\n## 4. Do I need Node.js and @lmstudio/sdk?", "metadata": {}}
{"id": "101", "text": "**If you still see duplicates:**\n- Update to the latest LEANN version\n- Restart LM Studio to clear loaded models\n- Check that you have JIT auto-evict enabled in LM Studio settings\n\n**How it works now:**\n1. LEANN loads model temporarily to get context length\n2. Immediately unloads after query\n3. LM Studio JIT loads model on-demand for actual embeddings\n4. Auto-evicts per your settings\n\n## 4. Do I need Node.js and @lmstudio/sdk?\n\n**No, it's completely optional.** LEANN works perfectly fine without them using a built-in token limit registry.\n\n**Benefits if you install it:**\n- Automatic context length detection for LM Studio models\n- No manual registry maintenance\n- Always gets accurate token limits from the model itself\n\n**To install (optional):**\n```bash\nnpm install -g @lmstudio/sdk\n```\n\nSee [Configuration Guide: LM Studio Auto-Detection](configuration-guide.md#lm-studio-auto-detection-optional) for details.", "metadata": {}}
{"id": "102", "text": "# ‚ú® Detailed Features\n\n## üî• Core Features\n\n- **üîÑ Real-time Embeddings** - Eliminate heavy embedding storage with dynamic computation using optimized ZMQ servers and highly optimized search paradigm (overlapping and batching) with highly optimized embedding engine\n- **üß† AST-Aware Code Chunking** - Intelligent code chunking that preserves semantic boundaries (functions, classes, methods) for Python, Java, C#, and TypeScript files\n- **üìà Scalable Architecture** - Handles millions of documents on consumer hardware; the larger your dataset, the more LEANN can save\n- **üéØ Graph Pruning** - Advanced techniques to minimize the storage overhead of vector search to a limited footprint\n- **üèóÔ∏è Pluggable Backends** - HNSW/FAISS (default), with optional DiskANN for large-scale deployments", "metadata": {}}
{"id": "103", "text": "## üõ†Ô∏è Technical Highlights\n- **üîÑ Recompute Mode** - Highest accuracy scenarios while eliminating vector storage overhead\n- **‚ö° Zero-copy Operations** - Minimize IPC overhead by transferring distances instead of embeddings\n- **üöÄ High-throughput Embedding Pipeline** - Optimized batched processing for maximum efficiency\n- **üéØ Two-level Search** - Novel coarse-to-fine search overlap for accelerated query processing (optional)\n- **üíæ Memory-mapped Indices** - Fast startup with raw text mapping to reduce memory overhead\n- **üöÄ MLX Support** - Ultra-fast recompute/build with quantized embedding models, accelerating building and search ([minimal example](../examples/mlx_demo.py))\n\n## üé® Developer Experience\n\n- **Simple Python API** - Get started in minutes\n- **Extensible backend system** - Easy to add new algorithms\n- **Comprehensive examples** - From basic usage to production deployment", "metadata": {}}
{"id": "104", "text": "# LEANN Grep Search Usage Guide\n\n## Overview\n\nLEANN's grep search functionality provides exact text matching for finding specific code patterns, error messages, function names, or exact phrases in your indexed documents.\n\n## Basic Usage\n\n### Simple Grep Search\n\n```python\nfrom leann.api import LeannSearcher\n\nsearcher = LeannSearcher(\"your_index_path\")\n\n# Exact text search\nresults = searcher.search(\"def authenticate_user\", use_grep=True, top_k=5)\n\nfor result in results:\n    print(f\"Score: {result.score}\")\n    print(f\"Text: {result.text[:100]}...\")\n    print(\"-\" * 40)\n```\n\n### Comparison: Semantic vs Grep Search\n\n```python\n# Semantic search - finds conceptually similar content\nsemantic_results = searcher.search(\"machine learning algorithms\", top_k=3)\n\n# Grep search - finds exact text matches\ngrep_results = searcher.search(\"def train_model\", use_grep=True, top_k=3)\n```\n\n## When to Use Grep Search\n\n### Use Cases", "metadata": {}}
{"id": "105", "text": "for result in results:\n    print(f\"Score: {result.score}\")\n    print(f\"Text: {result.text[:100]}...\")\n    print(\"-\" * 40)\n```\n\n### Comparison: Semantic vs Grep Search\n\n```python\n# Semantic search - finds conceptually similar content\nsemantic_results = searcher.search(\"machine learning algorithms\", top_k=3)\n\n# Grep search - finds exact text matches\ngrep_results = searcher.search(\"def train_model\", use_grep=True, top_k=3)\n```\n\n## When to Use Grep Search\n\n### Use Cases\n\n- **Code Search**: Finding specific function definitions, class names, or variable references\n- **Error Debugging**: Locating exact error messages or stack traces\n- **Documentation**: Finding specific API endpoints or exact terminology\n\n### Examples\n\n```python\n# Find function definitions\nfunctions = searcher.search(\"def __init__\", use_grep=True)\n\n# Find import statements\nimports = searcher.search(\"from sklearn import\", use_grep=True)\n\n# Find specific error types\nerrors = searcher.search(\"FileNotFoundError\", use_grep=True)", "metadata": {}}
{"id": "106", "text": "## When to Use Grep Search\n\n### Use Cases\n\n- **Code Search**: Finding specific function definitions, class names, or variable references\n- **Error Debugging**: Locating exact error messages or stack traces\n- **Documentation**: Finding specific API endpoints or exact terminology\n\n### Examples\n\n```python\n# Find function definitions\nfunctions = searcher.search(\"def __init__\", use_grep=True)\n\n# Find import statements\nimports = searcher.search(\"from sklearn import\", use_grep=True)\n\n# Find specific error types\nerrors = searcher.search(\"FileNotFoundError\", use_grep=True)\n\n# Find TODO comments\ntodos = searcher.search(\"TODO:\", use_grep=True)\n\n# Find configuration entries\nconfigs = searcher.search(\"server_port=\", use_grep=True)\n```\n\n## Technical Details\n\n### How It Works", "metadata": {}}
{"id": "107", "text": "### Examples\n\n```python\n# Find function definitions\nfunctions = searcher.search(\"def __init__\", use_grep=True)\n\n# Find import statements\nimports = searcher.search(\"from sklearn import\", use_grep=True)\n\n# Find specific error types\nerrors = searcher.search(\"FileNotFoundError\", use_grep=True)\n\n# Find TODO comments\ntodos = searcher.search(\"TODO:\", use_grep=True)\n\n# Find configuration entries\nconfigs = searcher.search(\"server_port=\", use_grep=True)\n```\n\n## Technical Details\n\n### How It Works\n\n1. **File Location**: Grep search operates on the raw text stored in `.jsonl` files\n2. **Command Execution**: Uses the system `grep` command with case-insensitive search\n3. **Result Processing**: Parses JSON lines and extracts text and metadata\n4. **Scoring**: Simple frequency-based scoring based on query term occurrences\n\n### Search Process", "metadata": {}}
{"id": "108", "text": "# Find TODO comments\ntodos = searcher.search(\"TODO:\", use_grep=True)\n\n# Find configuration entries\nconfigs = searcher.search(\"server_port=\", use_grep=True)\n```\n\n## Technical Details\n\n### How It Works\n\n1. **File Location**: Grep search operates on the raw text stored in `.jsonl` files\n2. **Command Execution**: Uses the system `grep` command with case-insensitive search\n3. **Result Processing**: Parses JSON lines and extracts text and metadata\n4. **Scoring**: Simple frequency-based scoring based on query term occurrences\n\n### Search Process\n\n```\nQuery: \"def train_model\"\n  ‚Üì\ngrep -i -n \"def train_model\" documents.leann.passages.jsonl\n  ‚Üì\nParse matching JSON lines\n  ‚Üì\nCalculate scores based on term frequency\n  ‚Üì\nReturn top_k results\n```\n\n### Scoring Algorithm\n\n```python\n# Term frequency in document\nscore = text.lower().count(query.lower())\n```\n\nResults are ranked by score (highest first), with higher scores indicating more occurrences of the search term.\n\n## Error Handling\n\n### Common Issues", "metadata": {}}
{"id": "109", "text": "### Search Process\n\n```\nQuery: \"def train_model\"\n  ‚Üì\ngrep -i -n \"def train_model\" documents.leann.passages.jsonl\n  ‚Üì\nParse matching JSON lines\n  ‚Üì\nCalculate scores based on term frequency\n  ‚Üì\nReturn top_k results\n```\n\n### Scoring Algorithm\n\n```python\n# Term frequency in document\nscore = text.lower().count(query.lower())\n```\n\nResults are ranked by score (highest first), with higher scores indicating more occurrences of the search term.\n\n## Error Handling\n\n### Common Issues\n\n#### Grep Command Not Found\n```\nRuntimeError: grep command not found. Please install grep or use semantic search.\n```\n\n**Solution**: Install grep on your system:\n- **Ubuntu/Debian**: `sudo apt-get install grep`\n- **macOS**: grep is pre-installed\n- **Windows**: Use WSL or install grep via Git Bash/MSYS2", "metadata": {}}
{"id": "110", "text": "```python\n# Term frequency in document\nscore = text.lower().count(query.lower())\n```\n\nResults are ranked by score (highest first), with higher scores indicating more occurrences of the search term.\n\n## Error Handling\n\n### Common Issues\n\n#### Grep Command Not Found\n```\nRuntimeError: grep command not found. Please install grep or use semantic search.\n```\n\n**Solution**: Install grep on your system:\n- **Ubuntu/Debian**: `sudo apt-get install grep`\n- **macOS**: grep is pre-installed\n- **Windows**: Use WSL or install grep via Git Bash/MSYS2\n\n#### No Results Found\n```python\n# Check if your query exists in the raw data\nresults = searcher.search(\"your_query\", use_grep=True)\nif not results:\n    print(\"No exact matches found. Try:\")\n    print(\"1. Check spelling and case\")\n    print(\"2. Use partial terms\")\n    print(\"3. Switch to semantic search\")\n```\n\n## Complete Example", "metadata": {}}
{"id": "111", "text": "#### No Results Found\n```python\n# Check if your query exists in the raw data\nresults = searcher.search(\"your_query\", use_grep=True)\nif not results:\n    print(\"No exact matches found. Try:\")\n    print(\"1. Check spelling and case\")\n    print(\"2. Use partial terms\")\n    print(\"3. Switch to semantic search\")\n```\n\n## Complete Example\n\n```python\n#!/usr/bin/env python3\n\"\"\"\nGrep Search Example\nDemonstrates grep search for exact text matching.\n\"\"\"\n\nfrom leann.api import LeannSearcher\n\ndef demonstrate_grep_search():\n    # Initialize searcher\n    searcher = LeannSearcher(\"my_index\")\n\n    print(\"=== Function Search ===\")\n    functions = searcher.search(\"def __init__\", use_grep=True, top_k=5)\n    for i, result in enumerate(functions, 1):\n        print(f\"{i}. Score: {result.score}\")\n        print(f\"   Preview: {result.text[:60]}...\")\n        print()", "metadata": {}}
{"id": "112", "text": "from leann.api import LeannSearcher\n\ndef demonstrate_grep_search():\n    # Initialize searcher\n    searcher = LeannSearcher(\"my_index\")\n\n    print(\"=== Function Search ===\")\n    functions = searcher.search(\"def __init__\", use_grep=True, top_k=5)\n    for i, result in enumerate(functions, 1):\n        print(f\"{i}. Score: {result.score}\")\n        print(f\"   Preview: {result.text[:60]}...\")\n        print()\n\n    print(\"=== Error Search ===\")\n    errors = searcher.search(\"FileNotFoundError\", use_grep=True, top_k=3)\n    for result in errors:\n        print(f\"Content: {result.text.strip()}\")\n        print(\"-\" * 40)\n\nif __name__ == \"__main__\":\n    demonstrate_grep_search()\n```", "metadata": {}}
{"id": "113", "text": "# LEANN Metadata Filtering Usage Guide\n\n## Overview\n\nLeann possesses metadata filtering capabilities that allow you to filter search results based on arbitrary metadata fields set during chunking. This feature enables use cases like spoiler-free book search, document filtering by date/type, code search by file type, and potentially much more.\n\n## Basic Usage\n\n### Adding Metadata to Your Documents\n\nWhen building your index, add metadata to each text chunk:\n\n```python\nfrom leann.api import LeannBuilder\n\nbuilder = LeannBuilder(\"hnsw\")\n\n# Add text with metadata\nbuilder.add_text(\n    text=\"Chapter 1: Alice falls down the rabbit hole\",\n    metadata={\n        \"chapter\": 1,\n        \"character\": \"Alice\",\n        \"themes\": [\"adventure\", \"curiosity\"],\n        \"word_count\": 150\n    }\n)\n\nbuilder.build_index(\"alice_in_wonderland_index\")\n```\n\n### Searching with Metadata Filters\n\nUse the `metadata_filters` parameter in search calls:\n\n```python\nfrom leann.api import LeannSearcher\n\nsearcher = LeannSearcher(\"alice_in_wonderland_index\")", "metadata": {}}
{"id": "114", "text": "# Add text with metadata\nbuilder.add_text(\n    text=\"Chapter 1: Alice falls down the rabbit hole\",\n    metadata={\n        \"chapter\": 1,\n        \"character\": \"Alice\",\n        \"themes\": [\"adventure\", \"curiosity\"],\n        \"word_count\": 150\n    }\n)\n\nbuilder.build_index(\"alice_in_wonderland_index\")\n```\n\n### Searching with Metadata Filters\n\nUse the `metadata_filters` parameter in search calls:\n\n```python\nfrom leann.api import LeannSearcher\n\nsearcher = LeannSearcher(\"alice_in_wonderland_index\")\n\n# Search with filters\nresults = searcher.search(\n    query=\"What happens to Alice?\",\n    top_k=10,\n    metadata_filters={\n        \"chapter\": {\"<=\": 5},           # Only chapters 1-5\n        \"spoiler_level\": {\"!=\": \"high\"} # No high spoilers\n    }\n)\n```\n\n## Filter Syntax\n\n### Basic Structure\n\n```python\nmetadata_filters = {\n    \"field_name\": {\"operator\": value},\n    \"another_field\": {\"operator\": value}\n}\n```", "metadata": {}}
{"id": "115", "text": "searcher = LeannSearcher(\"alice_in_wonderland_index\")\n\n# Search with filters\nresults = searcher.search(\n    query=\"What happens to Alice?\",\n    top_k=10,\n    metadata_filters={\n        \"chapter\": {\"<=\": 5},           # Only chapters 1-5\n        \"spoiler_level\": {\"!=\": \"high\"} # No high spoilers\n    }\n)\n```\n\n## Filter Syntax\n\n### Basic Structure\n\n```python\nmetadata_filters = {\n    \"field_name\": {\"operator\": value},\n    \"another_field\": {\"operator\": value}\n}\n```\n\n### Supported Operators\n\n#### Comparison Operators\n- `\"==\"`: Equal to\n- `\"!=\"`: Not equal to\n- `\"<\"`: Less than\n- `\"<=\"`: Less than or equal\n- `\">\"`: Greater than\n- `\">=\"`: Greater than or equal", "metadata": {}}
{"id": "116", "text": "## Filter Syntax\n\n### Basic Structure\n\n```python\nmetadata_filters = {\n    \"field_name\": {\"operator\": value},\n    \"another_field\": {\"operator\": value}\n}\n```\n\n### Supported Operators\n\n#### Comparison Operators\n- `\"==\"`: Equal to\n- `\"!=\"`: Not equal to\n- `\"<\"`: Less than\n- `\"<=\"`: Less than or equal\n- `\">\"`: Greater than\n- `\">=\"`: Greater than or equal\n\n```python\n# Examples\n{\"chapter\": {\"==\": 1}}           # Exactly chapter 1\n{\"page\": {\">\": 100}}            # Pages after 100\n{\"rating\": {\">=\": 4.0}}         # Rating 4.0 or higher\n{\"word_count\": {\"<\": 500}}      # Short passages\n```\n\n#### Membership Operators\n- `\"in\"`: Value is in list\n- `\"not_in\"`: Value is not in list", "metadata": {}}
{"id": "117", "text": "```python\n# Examples\n{\"chapter\": {\"==\": 1}}           # Exactly chapter 1\n{\"page\": {\">\": 100}}            # Pages after 100\n{\"rating\": {\">=\": 4.0}}         # Rating 4.0 or higher\n{\"word_count\": {\"<\": 500}}      # Short passages\n```\n\n#### Membership Operators\n- `\"in\"`: Value is in list\n- `\"not_in\"`: Value is not in list\n\n```python\n# Examples\n{\"character\": {\"in\": [\"Alice\", \"Bob\"]}}      # Alice OR Bob\n{\"genre\": {\"not_in\": [\"horror\", \"thriller\"]}} # Exclude genres\n{\"tags\": {\"in\": [\"fiction\", \"adventure\"]}}   # Any of these tags\n```\n\n#### String Operators\n- `\"contains\"`: String contains substring\n- `\"starts_with\"`: String starts with prefix\n- `\"ends_with\"`: String ends with suffix", "metadata": {}}
{"id": "118", "text": "```python\n# Examples\n{\"character\": {\"in\": [\"Alice\", \"Bob\"]}}      # Alice OR Bob\n{\"genre\": {\"not_in\": [\"horror\", \"thriller\"]}} # Exclude genres\n{\"tags\": {\"in\": [\"fiction\", \"adventure\"]}}   # Any of these tags\n```\n\n#### String Operators\n- `\"contains\"`: String contains substring\n- `\"starts_with\"`: String starts with prefix\n- `\"ends_with\"`: String ends with suffix\n\n```python\n# Examples\n{\"title\": {\"contains\": \"alice\"}}        # Title contains \"alice\"\n{\"filename\": {\"ends_with\": \".py\"}}      # Python files\n{\"author\": {\"starts_with\": \"Dr.\"}}      # Authors with \"Dr.\" prefix\n```\n\n#### Boolean Operators\n- `\"is_true\"`: Field is truthy\n- `\"is_false\"`: Field is falsy\n\n```python\n# Examples\n{\"is_published\": {\"is_true\": True}}     # Published content\n{\"is_draft\": {\"is_false\": False}}       # Not drafts\n```\n\n### Multiple Operators on Same Field", "metadata": {}}
{"id": "119", "text": "#### Boolean Operators\n- `\"is_true\"`: Field is truthy\n- `\"is_false\"`: Field is falsy\n\n```python\n# Examples\n{\"is_published\": {\"is_true\": True}}     # Published content\n{\"is_draft\": {\"is_false\": False}}       # Not drafts\n```\n\n### Multiple Operators on Same Field\n\nYou can apply multiple operators to the same field (AND logic):\n\n```python\nmetadata_filters = {\n    \"word_count\": {\n        \">=\": 100,    # At least 100 words\n        \"<=\": 500     # At most 500 words\n    }\n}\n```\n\n### Compound Filters\n\nMultiple fields are combined with AND logic:\n\n```python\nmetadata_filters = {\n    \"chapter\": {\"<=\": 10},              # Up to chapter 10\n    \"character\": {\"==\": \"Alice\"},       # About Alice\n    \"spoiler_level\": {\"!=\": \"high\"}     # No major spoilers\n}\n```\n\n## Use Case Examples\n\n### 1. Spoiler-Free Book Search", "metadata": {}}
{"id": "120", "text": "### Compound Filters\n\nMultiple fields are combined with AND logic:\n\n```python\nmetadata_filters = {\n    \"chapter\": {\"<=\": 10},              # Up to chapter 10\n    \"character\": {\"==\": \"Alice\"},       # About Alice\n    \"spoiler_level\": {\"!=\": \"high\"}     # No major spoilers\n}\n```\n\n## Use Case Examples\n\n### 1. Spoiler-Free Book Search\n\n```python\n# Reader has only read up to chapter 5\ndef search_spoiler_free(query, max_chapter):\n    return searcher.search(\n        query=query,\n        metadata_filters={\n            \"chapter\": {\"<=\": max_chapter},\n            \"spoiler_level\": {\"in\": [\"none\", \"low\"]}\n        }\n    )\n\nresults = search_spoiler_free(\"What happens to Alice?\", max_chapter=5)\n```\n\n### 2. Document Management by Date", "metadata": {}}
{"id": "121", "text": "## Use Case Examples\n\n### 1. Spoiler-Free Book Search\n\n```python\n# Reader has only read up to chapter 5\ndef search_spoiler_free(query, max_chapter):\n    return searcher.search(\n        query=query,\n        metadata_filters={\n            \"chapter\": {\"<=\": max_chapter},\n            \"spoiler_level\": {\"in\": [\"none\", \"low\"]}\n        }\n    )\n\nresults = search_spoiler_free(\"What happens to Alice?\", max_chapter=5)\n```\n\n### 2. Document Management by Date\n\n```python\n# Find recent documents\nrecent_docs = searcher.search(\n    query=\"project updates\",\n    metadata_filters={\n        \"date\": {\">=\": \"2024-01-01\"},\n        \"document_type\": {\"==\": \"report\"}\n    }\n)\n```\n\n### 3. Code Search by File Type", "metadata": {}}
{"id": "122", "text": "results = search_spoiler_free(\"What happens to Alice?\", max_chapter=5)\n```\n\n### 2. Document Management by Date\n\n```python\n# Find recent documents\nrecent_docs = searcher.search(\n    query=\"project updates\",\n    metadata_filters={\n        \"date\": {\">=\": \"2024-01-01\"},\n        \"document_type\": {\"==\": \"report\"}\n    }\n)\n```\n\n### 3. Code Search by File Type\n\n```python\n# Search only Python files\npython_code = searcher.search(\n    query=\"authentication function\",\n    metadata_filters={\n        \"file_extension\": {\"==\": \".py\"},\n        \"lines_of_code\": {\"<\": 100}\n    }\n)\n```\n\n### 4. Content Filtering by Audience\n\n```python\n# Age-appropriate content\nfamily_content = searcher.search(\n    query=\"adventure stories\",\n    metadata_filters={\n        \"age_rating\": {\"in\": [\"G\", \"PG\"]},\n        \"content_warnings\": {\"not_in\": [\"violence\", \"adult_themes\"]}\n    }\n)\n```\n\n### 5. Multi-Book Series Management", "metadata": {}}
{"id": "123", "text": "### 4. Content Filtering by Audience\n\n```python\n# Age-appropriate content\nfamily_content = searcher.search(\n    query=\"adventure stories\",\n    metadata_filters={\n        \"age_rating\": {\"in\": [\"G\", \"PG\"]},\n        \"content_warnings\": {\"not_in\": [\"violence\", \"adult_themes\"]}\n    }\n)\n```\n\n### 5. Multi-Book Series Management\n\n```python\n# Search across first 3 books only\nearly_series = searcher.search(\n    query=\"character development\",\n    metadata_filters={\n        \"series\": {\"==\": \"Harry Potter\"},\n        \"book_number\": {\"<=\": 3}\n    }\n)\n```\n\n## Running the Example\n\nYou can see metadata filtering in action with our spoiler-free book RAG example:\n\n```bash\n# Don't forget to set up the environment\nuv venv\nsource .venv/bin/activate\n\n# Set your OpenAI API key (required for embeddings, but you can update the example locally and use ollama instead)\nexport OPENAI_API_KEY=\"your-api-key-here\"", "metadata": {}}
{"id": "124", "text": "## Running the Example\n\nYou can see metadata filtering in action with our spoiler-free book RAG example:\n\n```bash\n# Don't forget to set up the environment\nuv venv\nsource .venv/bin/activate\n\n# Set your OpenAI API key (required for embeddings, but you can update the example locally and use ollama instead)\nexport OPENAI_API_KEY=\"your-api-key-here\"\n\n# Run the spoiler-free book RAG example\nuv run examples/spoiler_free_book_rag.py\n```\n\nThis example demonstrates:\n- Building an index with metadata (chapter numbers, characters, themes, locations)\n- Searching with filters to avoid spoilers (e.g., only show results up to chapter 5)\n- Different scenarios for readers at various points in the book\n\nThe example uses Alice's Adventures in Wonderland as sample data and shows how you can search for information without revealing plot points from later chapters.\n\n## Advanced Patterns\n\n### Custom Chunking with metadata\n\n```python\ndef chunk_book_with_metadata(book_text, book_info):\n    chunks = []", "metadata": {}}
{"id": "125", "text": "This example demonstrates:\n- Building an index with metadata (chapter numbers, characters, themes, locations)\n- Searching with filters to avoid spoilers (e.g., only show results up to chapter 5)\n- Different scenarios for readers at various points in the book\n\nThe example uses Alice's Adventures in Wonderland as sample data and shows how you can search for information without revealing plot points from later chapters.\n\n## Advanced Patterns\n\n### Custom Chunking with metadata\n\n```python\ndef chunk_book_with_metadata(book_text, book_info):\n    chunks = []\n\n    for chapter_num, chapter_text in parse_chapters(book_text):\n        # Extract entities, themes, etc.\n        characters = extract_characters(chapter_text)\n        themes = classify_themes(chapter_text)\n        spoiler_level = assess_spoiler_level(chapter_text, chapter_num)", "metadata": {}}
{"id": "126", "text": "The example uses Alice's Adventures in Wonderland as sample data and shows how you can search for information without revealing plot points from later chapters.\n\n## Advanced Patterns\n\n### Custom Chunking with metadata\n\n```python\ndef chunk_book_with_metadata(book_text, book_info):\n    chunks = []\n\n    for chapter_num, chapter_text in parse_chapters(book_text):\n        # Extract entities, themes, etc.\n        characters = extract_characters(chapter_text)\n        themes = classify_themes(chapter_text)\n        spoiler_level = assess_spoiler_level(chapter_text, chapter_num)\n\n        # Create chunks with rich metadata\n        for paragraph in split_paragraphs(chapter_text):\n            chunks.append({\n                \"text\": paragraph,\n                \"metadata\": {\n                    \"book_title\": book_info[\"title\"],\n                    \"chapter\": chapter_num,\n                    \"characters\": characters,\n                    \"themes\": themes,\n                    \"spoiler_level\": spoiler_level,\n                    \"word_count\": len(paragraph.split()),\n                    \"reading_level\": calculate_reading_level(paragraph)\n                }\n            })\n\n    return chunks\n```\n\n## Performance Considerations\n\n### Efficient Filtering Strategies", "metadata": {}}
{"id": "127", "text": "# Create chunks with rich metadata\n        for paragraph in split_paragraphs(chapter_text):\n            chunks.append({\n                \"text\": paragraph,\n                \"metadata\": {\n                    \"book_title\": book_info[\"title\"],\n                    \"chapter\": chapter_num,\n                    \"characters\": characters,\n                    \"themes\": themes,\n                    \"spoiler_level\": spoiler_level,\n                    \"word_count\": len(paragraph.split()),\n                    \"reading_level\": calculate_reading_level(paragraph)\n                }\n            })\n\n    return chunks\n```\n\n## Performance Considerations\n\n### Efficient Filtering Strategies\n\n1. **Post-search filtering**: Applies filters after vector search, which should be efficient for typical result sets (10-100 results).\n\n2. **Metadata design**: Keep metadata fields simple and avoid deeply nested structures.\n\n### Best Practices\n\n1. **Consistent metadata schema**: Use consistent field names and value types across your documents.\n\n2. **Reasonable metadata size**: Keep metadata reasonably sized to avoid storage overhead.\n\n3. **Type consistency**: Use consistent data types for the same fields (e.g., always integers for chapter numbers).", "metadata": {}}
{"id": "128", "text": "return chunks\n```\n\n## Performance Considerations\n\n### Efficient Filtering Strategies\n\n1. **Post-search filtering**: Applies filters after vector search, which should be efficient for typical result sets (10-100 results).\n\n2. **Metadata design**: Keep metadata fields simple and avoid deeply nested structures.\n\n### Best Practices\n\n1. **Consistent metadata schema**: Use consistent field names and value types across your documents.\n\n2. **Reasonable metadata size**: Keep metadata reasonably sized to avoid storage overhead.\n\n3. **Type consistency**: Use consistent data types for the same fields (e.g., always integers for chapter numbers).\n\n4. **Index multiple granularities**: Consider chunking at different levels (paragraph, section, chapter) with appropriate metadata.\n\n### Adding Metadata to Existing Indices\n\nTo add metadata filtering to existing indices, you'll need to rebuild them with metadata:\n\n```python\n# Read existing passages and add metadata\ndef add_metadata_to_existing_chunks(chunks):\n    for chunk in chunks:\n        # Extract or assign metadata based on content\n        chunk[\"metadata\"] = extract_metadata(chunk[\"text\"])\n    return chunks", "metadata": {}}
{"id": "129", "text": "4. **Index multiple granularities**: Consider chunking at different levels (paragraph, section, chapter) with appropriate metadata.\n\n### Adding Metadata to Existing Indices\n\nTo add metadata filtering to existing indices, you'll need to rebuild them with metadata:\n\n```python\n# Read existing passages and add metadata\ndef add_metadata_to_existing_chunks(chunks):\n    for chunk in chunks:\n        # Extract or assign metadata based on content\n        chunk[\"metadata\"] = extract_metadata(chunk[\"text\"])\n    return chunks\n\n# Rebuild index with metadata\nenhanced_chunks = add_metadata_to_existing_chunks(existing_chunks)\nbuilder = LeannBuilder(\"hnsw\")\nfor chunk in enhanced_chunks:\n    builder.add_text(chunk[\"text\"], chunk[\"metadata\"])\nbuilder.build_index(\"enhanced_index\")\n```", "metadata": {}}
{"id": "130", "text": "# Normalized Embeddings Support in LEANN\n\nLEANN now automatically detects normalized embedding models and sets the appropriate distance metric for optimal performance.\n\n## What are Normalized Embeddings?\n\nNormalized embeddings are vectors with L2 norm = 1 (unit vectors). These embeddings are optimized for cosine similarity rather than Maximum Inner Product Search (MIPS).\n\n## Automatic Detection\n\nWhen you create a `LeannBuilder` instance with a normalized embedding model, LEANN will:\n\n1. **Automatically set `distance_metric=\"cosine\"`** if not specified\n2. **Show a warning** if you manually specify a different distance metric\n3. **Provide optimal search performance** with the correct metric\n\n## Supported Normalized Embedding Models\n\n### OpenAI\nAll OpenAI text embedding models are normalized:\n- `text-embedding-ada-002`\n- `text-embedding-3-small`\n- `text-embedding-3-large`", "metadata": {}}
{"id": "131", "text": "## Automatic Detection\n\nWhen you create a `LeannBuilder` instance with a normalized embedding model, LEANN will:\n\n1. **Automatically set `distance_metric=\"cosine\"`** if not specified\n2. **Show a warning** if you manually specify a different distance metric\n3. **Provide optimal search performance** with the correct metric\n\n## Supported Normalized Embedding Models\n\n### OpenAI\nAll OpenAI text embedding models are normalized:\n- `text-embedding-ada-002`\n- `text-embedding-3-small`\n- `text-embedding-3-large`\n\n### Voyage AI\nAll Voyage AI embedding models are normalized:\n- `voyage-2`\n- `voyage-3`\n- `voyage-large-2`\n- `voyage-multilingual-2`\n- `voyage-code-2`\n\n### Cohere\nAll Cohere embedding models are normalized:\n- `embed-english-v3.0`\n- `embed-multilingual-v3.0`\n- `embed-english-light-v3.0`\n- `embed-multilingual-light-v3.0`\n\n## Example Usage", "metadata": {}}
{"id": "132", "text": "### Voyage AI\nAll Voyage AI embedding models are normalized:\n- `voyage-2`\n- `voyage-3`\n- `voyage-large-2`\n- `voyage-multilingual-2`\n- `voyage-code-2`\n\n### Cohere\nAll Cohere embedding models are normalized:\n- `embed-english-v3.0`\n- `embed-multilingual-v3.0`\n- `embed-english-light-v3.0`\n- `embed-multilingual-light-v3.0`\n\n## Example Usage\n\n```python\nfrom leann.api import LeannBuilder\n\n# Automatic detection - will use cosine distance\nbuilder = LeannBuilder(\n    backend_name=\"hnsw\",\n    embedding_model=\"text-embedding-3-small\",\n    embedding_mode=\"openai\"\n)\n# Warning: Detected normalized embeddings model 'text-embedding-3-small'...\n# Automatically setting distance_metric='cosine'", "metadata": {}}
{"id": "133", "text": "## Example Usage\n\n```python\nfrom leann.api import LeannBuilder\n\n# Automatic detection - will use cosine distance\nbuilder = LeannBuilder(\n    backend_name=\"hnsw\",\n    embedding_model=\"text-embedding-3-small\",\n    embedding_mode=\"openai\"\n)\n# Warning: Detected normalized embeddings model 'text-embedding-3-small'...\n# Automatically setting distance_metric='cosine'\n\n# Manual override (not recommended)\nbuilder = LeannBuilder(\n    backend_name=\"hnsw\",\n    embedding_model=\"text-embedding-3-small\",\n    embedding_mode=\"openai\",\n    distance_metric=\"mips\"  # Will show warning\n)\n# Warning: Using 'mips' distance metric with normalized embeddings...\n```\n\n## Non-Normalized Embeddings\n\nModels like `facebook/contriever` and other sentence-transformers models that are not normalized will continue to use MIPS by default, which is optimal for them.\n\n## Why This Matters", "metadata": {}}
{"id": "134", "text": "# Manual override (not recommended)\nbuilder = LeannBuilder(\n    backend_name=\"hnsw\",\n    embedding_model=\"text-embedding-3-small\",\n    embedding_mode=\"openai\",\n    distance_metric=\"mips\"  # Will show warning\n)\n# Warning: Using 'mips' distance metric with normalized embeddings...\n```\n\n## Non-Normalized Embeddings\n\nModels like `facebook/contriever` and other sentence-transformers models that are not normalized will continue to use MIPS by default, which is optimal for them.\n\n## Why This Matters\n\nUsing the wrong distance metric with normalized embeddings can lead to:\n- **Poor search quality** due to HNSW's early termination with narrow score ranges\n- **Incorrect ranking** of search results\n- **Suboptimal performance** compared to using the correct metric\n\nFor more details on why this happens, see our analysis in the [embedding detection code](../packages/leann-core/src/leann/api.py) which automatically handles normalized embeddings and MIPS distance metric issues.", "metadata": {}}
{"id": "135", "text": "# LEANN ReAct Agent Guide\n\n## Overview\n\nThe LEANN ReAct (Reasoning + Acting) Agent enables **multiturn retrieval and reasoning** for complex queries that require multiple search iterations. Unlike the standard `leann ask` command which performs a single search and answer, the ReAct agent can:\n\n- **Reason** about what information is needed\n- **Act** by performing targeted searches\n- **Observe** the results and iterate\n- **Answer** based on all gathered context\n\nThis is particularly useful for questions that require:\n- Multiple pieces of information from different parts of your index\n- Iterative refinement of search queries\n- Complex reasoning that builds on previous findings\n\n## How It Works\n\nThe ReAct agent follows a **Thought ‚Üí Action ‚Üí Observation** loop:", "metadata": {}}
{"id": "136", "text": "- **Reason** about what information is needed\n- **Act** by performing targeted searches\n- **Observe** the results and iterate\n- **Answer** based on all gathered context\n\nThis is particularly useful for questions that require:\n- Multiple pieces of information from different parts of your index\n- Iterative refinement of search queries\n- Complex reasoning that builds on previous findings\n\n## How It Works\n\nThe ReAct agent follows a **Thought ‚Üí Action ‚Üí Observation** loop:\n\n1. **Thought**: The agent analyzes the question and determines what information is needed\n2. **Action**: The agent performs a search query based on its reasoning\n3. **Observation**: The agent reviews the search results\n4. **Iteration**: The process repeats until the agent has enough information or reaches the maximum iteration limit\n5. **Final Answer**: The agent synthesizes all gathered information into a comprehensive answer\n\n## Basic Usage\n\n### Command Line\n\n```bash\n# Basic usage\nleann react <index_name> \"your question\"", "metadata": {}}
{"id": "137", "text": "The ReAct agent follows a **Thought ‚Üí Action ‚Üí Observation** loop:\n\n1. **Thought**: The agent analyzes the question and determines what information is needed\n2. **Action**: The agent performs a search query based on its reasoning\n3. **Observation**: The agent reviews the search results\n4. **Iteration**: The process repeats until the agent has enough information or reaches the maximum iteration limit\n5. **Final Answer**: The agent synthesizes all gathered information into a comprehensive answer\n\n## Basic Usage\n\n### Command Line\n\n```bash\n# Basic usage\nleann react <index_name> \"your question\"\n\n# With custom LLM settings\nleann react my-index \"What are the main features discussed?\" \\\n  --llm ollama \\\n  --model qwen3:8b \\\n  --max-iterations 5 \\\n  --top-k 5\n```\n\n### Command Options", "metadata": {}}
{"id": "138", "text": "## Basic Usage\n\n### Command Line\n\n```bash\n# Basic usage\nleann react <index_name> \"your question\"\n\n# With custom LLM settings\nleann react my-index \"What are the main features discussed?\" \\\n  --llm ollama \\\n  --model qwen3:8b \\\n  --max-iterations 5 \\\n  --top-k 5\n```\n\n### Command Options\n\n- `index_name`: Name of the LEANN index to search\n- `query`: The question to research\n- `--llm`: LLM provider (`ollama`, `openai`, `anthropic`, `hf`, `simulated`) - default: `ollama`\n- `--model`: Model name (default: `qwen3:8b`)\n- `--host`: Override Ollama-compatible host (defaults to `LEANN_OLLAMA_HOST` or `OLLAMA_HOST`)\n- `--top-k`: Number of results per search iteration (default: `5`)\n- `--max-iterations`: Maximum number of search iterations (default: `5`)\n- `--api-base`: Base URL for OpenAI-compatible APIs\n- `--api-key`: API key for cloud LLM providers", "metadata": {}}
{"id": "139", "text": "### Python API\n\n```python\nfrom leann import create_react_agent, LeannSearcher\n\n# Create a searcher\nsearcher = LeannSearcher(index_path=\"path/to/index.leann\")\n\n# Create the ReAct agent\nagent = create_react_agent(\n    index_path=\"path/to/index.leann\",\n    llm_config={\n        \"type\": \"ollama\",\n        \"model\": \"qwen3:8b\",\n        \"host\": \"http://localhost:11434\"  # optional\n    },\n    max_iterations=5\n)\n\n# Run the agent\nanswer = agent.run(\"What are the main topics covered in the documentation?\", top_k=5)\nprint(answer)\n\n# Access search history\nif agent.search_history:\n    print(f\"\\nSearch History ({len(agent.search_history)} iterations):\")\n    for entry in agent.search_history:\n        print(f\"  {entry['iteration']}. {entry['action']} ({entry['results_count']} results)\")\n```\n\n## Example Use Cases\n\n### 1. Multi-faceted Questions", "metadata": {}}
{"id": "140", "text": "# Run the agent\nanswer = agent.run(\"What are the main topics covered in the documentation?\", top_k=5)\nprint(answer)\n\n# Access search history\nif agent.search_history:\n    print(f\"\\nSearch History ({len(agent.search_history)} iterations):\")\n    for entry in agent.search_history:\n        print(f\"  {entry['iteration']}. {entry['action']} ({entry['results_count']} results)\")\n```\n\n## Example Use Cases\n\n### 1. Multi-faceted Questions\n\n```bash\n# Questions that need information from multiple sources\nleann react docs-index \"What are the differences between HNSW and DiskANN backends, and when should I use each?\"\n```\n\nThe agent will:\n- First search for \"HNSW backend features\"\n- Then search for \"DiskANN backend features\"\n- Compare the results\n- Provide a comprehensive answer\n\n### 2. Iterative Research\n\n```bash\n# Questions requiring multiple search iterations\nleann react codebase-index \"How does the embedding computation work and what optimizations are used?\"\n```", "metadata": {}}
{"id": "141", "text": "### 1. Multi-faceted Questions\n\n```bash\n# Questions that need information from multiple sources\nleann react docs-index \"What are the differences between HNSW and DiskANN backends, and when should I use each?\"\n```\n\nThe agent will:\n- First search for \"HNSW backend features\"\n- Then search for \"DiskANN backend features\"\n- Compare the results\n- Provide a comprehensive answer\n\n### 2. Iterative Research\n\n```bash\n# Questions requiring multiple search iterations\nleann react codebase-index \"How does the embedding computation work and what optimizations are used?\"\n```\n\nThe agent will:\n- Search for \"embedding computation\"\n- Based on results, search for \"embedding optimizations\"\n- Refine queries based on findings\n- Synthesize the information\n\n### 3. Complex Reasoning\n\n```bash\n# Questions that require building understanding\nleann react research-index \"What are the performance characteristics of different indexing strategies?\"\n```\n\n## Comparison: `leann ask` vs `leann react`", "metadata": {}}
{"id": "142", "text": "### 2. Iterative Research\n\n```bash\n# Questions requiring multiple search iterations\nleann react codebase-index \"How does the embedding computation work and what optimizations are used?\"\n```\n\nThe agent will:\n- Search for \"embedding computation\"\n- Based on results, search for \"embedding optimizations\"\n- Refine queries based on findings\n- Synthesize the information\n\n### 3. Complex Reasoning\n\n```bash\n# Questions that require building understanding\nleann react research-index \"What are the performance characteristics of different indexing strategies?\"\n```\n\n## Comparison: `leann ask` vs `leann react`\n\n| Feature | `leann ask` | `leann react` |\n|---------|-------------|---------------|\n| **Search iterations** | Single search | Multiple iterations |\n| **Query refinement** | No | Yes, based on observations |\n| **Use case** | Simple Q&A | Complex, multi-faceted questions |\n| **Speed** | Faster | Slower (multiple searches) |\n| **Reasoning** | Direct answer | Iterative reasoning |\n\n### When to Use Each", "metadata": {}}
{"id": "143", "text": "## Comparison: `leann ask` vs `leann react`\n\n| Feature | `leann ask` | `leann react` |\n|---------|-------------|---------------|\n| **Search iterations** | Single search | Multiple iterations |\n| **Query refinement** | No | Yes, based on observations |\n| **Use case** | Simple Q&A | Complex, multi-faceted questions |\n| **Speed** | Faster | Slower (multiple searches) |\n| **Reasoning** | Direct answer | Iterative reasoning |\n\n### When to Use Each\n\n**Use `leann ask` when:**\n- You have a straightforward question\n- A single search should provide enough context\n- You want a quick answer\n\n**Use `leann react` when:**\n- Your question requires information from multiple sources\n- You need the agent to explore and refine its understanding\n- The answer requires synthesizing multiple pieces of information\n\n## Advanced Configuration\n\n### Custom LLM Providers", "metadata": {}}
{"id": "144", "text": "### When to Use Each\n\n**Use `leann ask` when:**\n- You have a straightforward question\n- A single search should provide enough context\n- You want a quick answer\n\n**Use `leann react` when:**\n- Your question requires information from multiple sources\n- You need the agent to explore and refine its understanding\n- The answer requires synthesizing multiple pieces of information\n\n## Advanced Configuration\n\n### Custom LLM Providers\n\n```bash\n# Using OpenAI\nleann react my-index \"question\" \\\n  --llm openai \\\n  --model gpt-4 \\\n  --api-base https://api.openai.com/v1 \\\n  --api-key $OPENAI_API_KEY\n\n# Using Anthropic\nleann react my-index \"question\" \\\n  --llm anthropic \\\n  --model claude-3-opus-20240229 \\\n  --api-key $ANTHROPIC_API_KEY\n```\n\n### Adjusting Search Parameters\n\n```bash\n# More results per iteration\nleann react my-index \"question\" --top-k 10", "metadata": {}}
{"id": "145", "text": "# Using Anthropic\nleann react my-index \"question\" \\\n  --llm anthropic \\\n  --model claude-3-opus-20240229 \\\n  --api-key $ANTHROPIC_API_KEY\n```\n\n### Adjusting Search Parameters\n\n```bash\n# More results per iteration\nleann react my-index \"question\" --top-k 10\n\n# More iterations for complex questions\nleann react my-index \"question\" --max-iterations 10\n```\n\n## Understanding the Output\n\nWhen you run `leann react`, you'll see:\n\n1. **Question**: The original question being researched\n2. **Iteration logs**: Each search action and its results\n3. **Final Answer**: The synthesized answer based on all iterations\n4. **Search History**: Summary of all search iterations performed\n\nExample output:\n\n```\nü§ñ Starting ReAct agent with index 'my-index'...\nUsing qwen3:8b (ollama)\n\nüîç Question: What are the main features of LEANN?", "metadata": {}}
{"id": "146", "text": "## Understanding the Output\n\nWhen you run `leann react`, you'll see:\n\n1. **Question**: The original question being researched\n2. **Iteration logs**: Each search action and its results\n3. **Final Answer**: The synthesized answer based on all iterations\n4. **Search History**: Summary of all search iterations performed\n\nExample output:\n\n```\nü§ñ Starting ReAct agent with index 'my-index'...\nUsing qwen3:8b (ollama)\n\nüîç Question: What are the main features of LEANN?\n\nüîç Action: search(\"LEANN features\")\n[Result 1] (Score: 0.923)\nLEANN is a vector database that saves 97% storage...\n\nüîç Action: search(\"LEANN storage optimization\")\n[Result 1] (Score: 0.891)\nLEANN uses compact storage and recomputation...", "metadata": {}}
{"id": "147", "text": "Example output:\n\n```\nü§ñ Starting ReAct agent with index 'my-index'...\nUsing qwen3:8b (ollama)\n\nüîç Question: What are the main features of LEANN?\n\nüîç Action: search(\"LEANN features\")\n[Result 1] (Score: 0.923)\nLEANN is a vector database that saves 97% storage...\n\nüîç Action: search(\"LEANN storage optimization\")\n[Result 1] (Score: 0.891)\nLEANN uses compact storage and recomputation...\n\n‚úÖ Final Answer:\nLEANN is a vector database with several key features:\n1. 97% storage savings compared to traditional vector databases\n2. Compact storage with recomputation capabilities\n3. Support for multiple backends (HNSW and DiskANN)\n...\n\nüìä Search History (2 iterations):\n  1. search(\"LEANN features\") (5 results)\n  2. search(\"LEANN storage optimization\") (5 results)\n```\n\n## Tips for Best Results", "metadata": {}}
{"id": "148", "text": "‚úÖ Final Answer:\nLEANN is a vector database with several key features:\n1. 97% storage savings compared to traditional vector databases\n2. Compact storage with recomputation capabilities\n3. Support for multiple backends (HNSW and DiskANN)\n...\n\nüìä Search History (2 iterations):\n  1. search(\"LEANN features\") (5 results)\n  2. search(\"LEANN storage optimization\") (5 results)\n```\n\n## Tips for Best Results\n\n1. **Be specific**: Clear, specific questions work better than vague ones\n2. **Adjust iterations**: Complex questions may need more iterations (increase `--max-iterations`)\n3. **Monitor history**: Check the search history to understand the agent's reasoning\n4. **Use appropriate models**: Larger models generally provide better reasoning, but are slower\n5. **Index quality**: Ensure your index is well-built with relevant content\n\n## Limitations", "metadata": {}}
{"id": "149", "text": "## Tips for Best Results\n\n1. **Be specific**: Clear, specific questions work better than vague ones\n2. **Adjust iterations**: Complex questions may need more iterations (increase `--max-iterations`)\n3. **Monitor history**: Check the search history to understand the agent's reasoning\n4. **Use appropriate models**: Larger models generally provide better reasoning, but are slower\n5. **Index quality**: Ensure your index is well-built with relevant content\n\n## Limitations\n\n- **Speed**: Multiple iterations make ReAct slower than single-search queries\n- **Cost**: More LLM calls mean higher costs for cloud providers\n- **Complexity**: Very complex questions may still require human review\n- **Model dependency**: Reasoning quality depends on the LLM's capabilities\n\n## Future Enhancements\n\nThis is the first implementation (1/N) of Deep-Research integration. Future enhancements may include:\n- Web search integration for external information\n- More sophisticated reasoning strategies\n- Parallel search execution\n- Better query optimization\n\n## Related Documentation", "metadata": {}}
{"id": "150", "text": "## Limitations\n\n- **Speed**: Multiple iterations make ReAct slower than single-search queries\n- **Cost**: More LLM calls mean higher costs for cloud providers\n- **Complexity**: Very complex questions may still require human review\n- **Model dependency**: Reasoning quality depends on the LLM's capabilities\n\n## Future Enhancements\n\nThis is the first implementation (1/N) of Deep-Research integration. Future enhancements may include:\n- Web search integration for external information\n- More sophisticated reasoning strategies\n- Parallel search execution\n- Better query optimization\n\n## Related Documentation\n\n- [Basic Usage Guide](../README.md)\n- [CLI Reference](configuration-guide.md)\n- [Embedding Models](normalized_embeddings.md)", "metadata": {}}
{"id": "151", "text": "# üìà Roadmap\n\n## üéØ Q2 2025\n\n- [X] HNSW backend integration\n- [X] DiskANN backend with MIPS/L2/Cosine support\n- [X] Real-time embedding pipeline\n- [X] Memory-efficient graph pruning\n\n## üöÄ Q3 2025\n\n- [ ] Advanced caching strategies\n- [ ] Add contextual-retrieval https://www.anthropic.com/news/contextual-retrieval\n- [ ] Add sleep-time-compute and summarize agent! to summarilze the file on computer!\n- [ ] Add OpenAI recompute API\n\n## üåü Q4 2025\n\n- [ ] Integration with LangChain/LlamaIndex\n- [ ] Visual similarity search\n- [ ] Query rewrtiting, rerank and expansion", "metadata": {}}
{"id": "152", "text": "# Slack Integration Setup Guide\n\nThis guide provides step-by-step instructions for setting up Slack integration with LEANN.\n\n## Overview\n\nLEANN's Slack integration uses MCP (Model Context Protocol) servers to fetch and index your Slack messages for RAG (Retrieval-Augmented Generation). This allows you to search through your Slack conversations using natural language queries.\n\n## Prerequisites\n\n1. **Slack Workspace Access**: You need admin or owner permissions in your Slack workspace to create apps and configure OAuth tokens.\n\n2. **Slack MCP Server**: Install a Slack MCP server (e.g., `slack-mcp-server` via npm)\n\n3. **LEANN**: Ensure you have LEANN installed and working\n\n## Step 1: Create a Slack App\n\n### 1.1 Go to Slack API Dashboard", "metadata": {}}
{"id": "153", "text": "## Prerequisites\n\n1. **Slack Workspace Access**: You need admin or owner permissions in your Slack workspace to create apps and configure OAuth tokens.\n\n2. **Slack MCP Server**: Install a Slack MCP server (e.g., `slack-mcp-server` via npm)\n\n3. **LEANN**: Ensure you have LEANN installed and working\n\n## Step 1: Create a Slack App\n\n### 1.1 Go to Slack API Dashboard\n\n1. Visit [https://api.slack.com/apps](https://api.slack.com/apps)\n2. Click **\"Create New App\"**\n3. Choose **\"From scratch\"**\n4. Enter your app name (e.g., \"LEANN Slack Integration\")\n5. Select your workspace\n6. Click **\"Create App\"**\n\n### 1.2 Configure App Permissions\n\n#### Token Scopes", "metadata": {}}
{"id": "154", "text": "3. **LEANN**: Ensure you have LEANN installed and working\n\n## Step 1: Create a Slack App\n\n### 1.1 Go to Slack API Dashboard\n\n1. Visit [https://api.slack.com/apps](https://api.slack.com/apps)\n2. Click **\"Create New App\"**\n3. Choose **\"From scratch\"**\n4. Enter your app name (e.g., \"LEANN Slack Integration\")\n5. Select your workspace\n6. Click **\"Create App\"**\n\n### 1.2 Configure App Permissions\n\n#### Token Scopes\n\n1. In your app dashboard, go to **\"OAuth & Permissions\"** in the left sidebar\n2. Scroll down to **\"Scopes\"** section\n3. Under **\"Bot Token Scopes & OAuth Scope\"**, click **\"Add an OAuth Scope\"**\n4. Add the following scopes:\n   - `channels:read` - Read public channel information\n   - `channels:history` - Read messages in public channels\n   - `groups:read` - Read private channel information\n   - `groups:history` - Read messages in private channels\n   - `im:read` - Read direct message information\n   - `im:history` - Read direct messages\n   - `mpim:read` - Read group direct message information\n   - `mpim:history` - Read group direct messages\n   - `users:read` - Read user information\n   - `team:read` - Read workspace information", "metadata": {}}
{"id": "155", "text": "#### App-Level Tokens (Optional)\n\nSome MCP servers may require app-level tokens:\n\n1. Go to **\"Basic Information\"** in the left sidebar\n2. Scroll down to **\"App-Level Tokens\"**\n3. Click **\"Generate Token and Scopes\"**\n4. Enter a name (e.g., \"LEANN Integration\")\n5. Add the `connections:write` scope\n6. Click **\"Generate\"**\n7. Copy the token (starts with `xapp-`)\n\n### 1.3 Install App to Workspace\n\n1. Go to **\"OAuth & Permissions\"** in the left sidebar\n2. Click **\"Install to Workspace\"**\n3. Review the permissions and click **\"Allow\"**\n4. Copy the **\"Bot User OAuth Token\"** (starts with `xoxb-`)\n5. Copy the **\"User OAuth Token\"** (starts with `xoxp-`)\n\n## Step 2: Install Slack MCP Server\n\n### Option A: Using npm (Recommended)\n\n```bash\n# Install globally\nnpm install -g slack-mcp-server", "metadata": {}}
{"id": "156", "text": "### 1.3 Install App to Workspace\n\n1. Go to **\"OAuth & Permissions\"** in the left sidebar\n2. Click **\"Install to Workspace\"**\n3. Review the permissions and click **\"Allow\"**\n4. Copy the **\"Bot User OAuth Token\"** (starts with `xoxb-`)\n5. Copy the **\"User OAuth Token\"** (starts with `xoxp-`)\n\n## Step 2: Install Slack MCP Server\n\n### Option A: Using npm (Recommended)\n\n```bash\n# Install globally\nnpm install -g slack-mcp-server\n\n# Or install locally\nnpm install slack-mcp-server\n```\n\n### Option B: Using npx (No installation required)\n\n```bash\n# Use directly without installation\nnpx slack-mcp-server\n```\n\n## Step 3: Install and Configure Ollama (for Real LLM Responses)\n\n### 3.1 Install Ollama\n\n```bash\n# Install Ollama using Homebrew (macOS)\nbrew install ollama\n\n# Or download from https://ollama.ai/\n```", "metadata": {}}
{"id": "157", "text": "```bash\n# Install globally\nnpm install -g slack-mcp-server\n\n# Or install locally\nnpm install slack-mcp-server\n```\n\n### Option B: Using npx (No installation required)\n\n```bash\n# Use directly without installation\nnpx slack-mcp-server\n```\n\n## Step 3: Install and Configure Ollama (for Real LLM Responses)\n\n### 3.1 Install Ollama\n\n```bash\n# Install Ollama using Homebrew (macOS)\nbrew install ollama\n\n# Or download from https://ollama.ai/\n```\n\n### 3.2 Start Ollama Service\n\n```bash\n# Start Ollama as a service\nbrew services start ollama\n\n# Or start manually\nollama serve\n```\n\n### 3.3 Pull a Model\n\n```bash\n# Pull a lightweight model for testing\nollama pull llama3.2:1b\n\n# Verify the model is available\nollama list\n```\n\n## Step 4: Configure Environment Variables\n\nCreate a `.env` file or set environment variables:", "metadata": {}}
{"id": "158", "text": "# Or download from https://ollama.ai/\n```\n\n### 3.2 Start Ollama Service\n\n```bash\n# Start Ollama as a service\nbrew services start ollama\n\n# Or start manually\nollama serve\n```\n\n### 3.3 Pull a Model\n\n```bash\n# Pull a lightweight model for testing\nollama pull llama3.2:1b\n\n# Verify the model is available\nollama list\n```\n\n## Step 4: Configure Environment Variables\n\nCreate a `.env` file or set environment variables:\n\n```bash\n# Required: User OAuth Token\nSLACK_OAUTH_TOKEN=xoxp-your-user-oauth-token-here\n\n# Optional: App-Level Token (if your MCP server requires it)\nSLACK_APP_TOKEN=xapp-your-app-token-here\n\n# Optional: Workspace-specific settings\nSLACK_WORKSPACE_ID=T1234567890  # Your workspace ID (optional)\n```\n\n## Step 5: Test the Setup\n\n### 5.1 Test MCP Server Connection", "metadata": {}}
{"id": "159", "text": "## Step 4: Configure Environment Variables\n\nCreate a `.env` file or set environment variables:\n\n```bash\n# Required: User OAuth Token\nSLACK_OAUTH_TOKEN=xoxp-your-user-oauth-token-here\n\n# Optional: App-Level Token (if your MCP server requires it)\nSLACK_APP_TOKEN=xapp-your-app-token-here\n\n# Optional: Workspace-specific settings\nSLACK_WORKSPACE_ID=T1234567890  # Your workspace ID (optional)\n```\n\n## Step 5: Test the Setup\n\n### 5.1 Test MCP Server Connection\n\n```bash\npython -m apps.slack_rag \\\n  --mcp-server \"slack-mcp-server\" \\\n  --test-connection \\\n  --workspace-name \"Your Workspace Name\"\n```\n\nThis will test the connection and list available tools without indexing any data.\n\n### 5.2 Index a Specific Channel", "metadata": {}}
{"id": "160", "text": "# Optional: Workspace-specific settings\nSLACK_WORKSPACE_ID=T1234567890  # Your workspace ID (optional)\n```\n\n## Step 5: Test the Setup\n\n### 5.1 Test MCP Server Connection\n\n```bash\npython -m apps.slack_rag \\\n  --mcp-server \"slack-mcp-server\" \\\n  --test-connection \\\n  --workspace-name \"Your Workspace Name\"\n```\n\nThis will test the connection and list available tools without indexing any data.\n\n### 5.2 Index a Specific Channel\n\n```bash\npython -m apps.slack_rag \\\n  --mcp-server \"slack-mcp-server\" \\\n  --workspace-name \"Your Workspace Name\" \\\n  --channels general \\\n  --query \"What did we discuss about the project?\"\n```\n\n### 5.3 Real RAG Query Examples\n\nThis section demonstrates successful Slack RAG integration queries against the Sky Lab Computing workspace's \"random\" channel. The system successfully retrieves actual conversation messages and performs semantic search with high relevance scores, including finding specific research paper announcements and technical discussions.\n\n### Example 1: Advisor Models Query", "metadata": {}}
{"id": "161", "text": "```bash\npython -m apps.slack_rag \\\n  --mcp-server \"slack-mcp-server\" \\\n  --workspace-name \"Your Workspace Name\" \\\n  --channels general \\\n  --query \"What did we discuss about the project?\"\n```\n\n### 5.3 Real RAG Query Examples\n\nThis section demonstrates successful Slack RAG integration queries against the Sky Lab Computing workspace's \"random\" channel. The system successfully retrieves actual conversation messages and performs semantic search with high relevance scores, including finding specific research paper announcements and technical discussions.\n\n### Example 1: Advisor Models Query\n\n**Query:** \"train black-box models to adopt to your personal data\"\n\nThis query demonstrates the system's ability to find specific research announcements about training black-box models for personal data adaptation.\n\n![Advisor Models Query - Command Setup](videos/slack_integration_1.1.png)\n\n![Advisor Models Query - Search Results](videos/slack_integration_1.2.png)\n\n![Advisor Models Query - LLM Response](videos/slack_integration_1.3.png)\n\n### Example 2: Barbarians at the Gate Query", "metadata": {}}
{"id": "162", "text": "### Example 1: Advisor Models Query\n\n**Query:** \"train black-box models to adopt to your personal data\"\n\nThis query demonstrates the system's ability to find specific research announcements about training black-box models for personal data adaptation.\n\n![Advisor Models Query - Command Setup](videos/slack_integration_1.1.png)\n\n![Advisor Models Query - Search Results](videos/slack_integration_1.2.png)\n\n![Advisor Models Query - LLM Response](videos/slack_integration_1.3.png)\n\n### Example 2: Barbarians at the Gate Query\n\n**Query:** \"AI-driven research systems ADRS\"\n\nThis query demonstrates the system's ability to find specific research announcements about AI-driven research systems and algorithm discovery.\n\n![Barbarians Query - Command Setup](videos/slack_integration_2.1.png)\n\n![Barbarians Query - Search Results](videos/slack_integration_2.2.png)\n\n![Barbarians Query - LLM Response](videos/slack_integration_2.3.png)\n\n### Prerequisites", "metadata": {}}
{"id": "163", "text": "### Example 2: Barbarians at the Gate Query\n\n**Query:** \"AI-driven research systems ADRS\"\n\nThis query demonstrates the system's ability to find specific research announcements about AI-driven research systems and algorithm discovery.\n\n![Barbarians Query - Command Setup](videos/slack_integration_2.1.png)\n\n![Barbarians Query - Search Results](videos/slack_integration_2.2.png)\n\n![Barbarians Query - LLM Response](videos/slack_integration_2.3.png)\n\n### Prerequisites\n\n- Bot is installed in the Sky Lab Computing workspace and invited to the target channel (run `/invite @YourBotName` in the channel if needed)\n- Bot token available and exported in the same terminal session\n\n### Commands\n\n1) Set the workspace token for this shell\n\n```bash\nexport SLACK_MCP_XOXP_TOKEN=\"xoxp-***-redacted-***\"\n```\n\n2) Run queries against the \"random\" channel by channel ID (C0GN5BX0F)", "metadata": {}}
{"id": "164", "text": "### Prerequisites\n\n- Bot is installed in the Sky Lab Computing workspace and invited to the target channel (run `/invite @YourBotName` in the channel if needed)\n- Bot token available and exported in the same terminal session\n\n### Commands\n\n1) Set the workspace token for this shell\n\n```bash\nexport SLACK_MCP_XOXP_TOKEN=\"xoxp-***-redacted-***\"\n```\n\n2) Run queries against the \"random\" channel by channel ID (C0GN5BX0F)\n\n**Advisor Models Query:**\n```bash\npython -m apps.slack_rag \\\n  --mcp-server \"slack-mcp-server\" \\\n  --workspace-name \"Sky Lab Computing\" \\\n  --channels C0GN5BX0F \\\n  --max-messages-per-channel 100000 \\\n  --query \"train black-box models to adopt to your personal data\" \\\n  --llm ollama \\\n  --llm-model \"llama3.2:1b\" \\\n  --llm-host \"http://localhost:11434\" \\\n  --no-concatenate-conversations\n```", "metadata": {}}
{"id": "165", "text": "**Advisor Models Query:**\n```bash\npython -m apps.slack_rag \\\n  --mcp-server \"slack-mcp-server\" \\\n  --workspace-name \"Sky Lab Computing\" \\\n  --channels C0GN5BX0F \\\n  --max-messages-per-channel 100000 \\\n  --query \"train black-box models to adopt to your personal data\" \\\n  --llm ollama \\\n  --llm-model \"llama3.2:1b\" \\\n  --llm-host \"http://localhost:11434\" \\\n  --no-concatenate-conversations\n```\n\n**Barbarians at the Gate Query:**\n```bash\npython -m apps.slack_rag \\\n  --mcp-server \"slack-mcp-server\" \\\n  --workspace-name \"Sky Lab Computing\" \\\n  --channels C0GN5BX0F \\\n  --max-messages-per-channel 100000 \\\n  --query \"AI-driven research systems ADRS\" \\\n  --llm ollama \\\n  --llm-model \"llama3.2:1b\" \\\n  --llm-host \"http://localhost:11434\" \\\n  --no-concatenate-conversations\n```", "metadata": {}}
{"id": "166", "text": "**Barbarians at the Gate Query:**\n```bash\npython -m apps.slack_rag \\\n  --mcp-server \"slack-mcp-server\" \\\n  --workspace-name \"Sky Lab Computing\" \\\n  --channels C0GN5BX0F \\\n  --max-messages-per-channel 100000 \\\n  --query \"AI-driven research systems ADRS\" \\\n  --llm ollama \\\n  --llm-model \"llama3.2:1b\" \\\n  --llm-host \"http://localhost:11434\" \\\n  --no-concatenate-conversations\n```\n\nThese examples demonstrate the system's ability to find and retrieve specific research announcements and technical discussions from the conversation history, showcasing the power of semantic search in Slack data.\n\n3) Optional: Ask a broader question\n\n```bash\npython test_channel_by_id_or_name.py \\\n  --channel-id C0GN5BX0F \\\n  --workspace-name \"Sky Lab Computing\" \\\n  --query \"What is LEANN about?\"\n```", "metadata": {}}
{"id": "167", "text": "These examples demonstrate the system's ability to find and retrieve specific research announcements and technical discussions from the conversation history, showcasing the power of semantic search in Slack data.\n\n3) Optional: Ask a broader question\n\n```bash\npython test_channel_by_id_or_name.py \\\n  --channel-id C0GN5BX0F \\\n  --workspace-name \"Sky Lab Computing\" \\\n  --query \"What is LEANN about?\"\n```\n\nNotes:\n- If you see `not_in_channel`, invite the bot to the channel and re-run.\n- If you see `channel_not_found`, confirm the channel ID and workspace.\n- Deep search via server-side ‚Äúsearch‚Äù tools may require additional Slack scopes; the example above performs client-side filtering over retrieved history.\n\n## Common Issues and Solutions\n\n### Issue 1: \"users cache is not ready yet\" Error\n\n**Problem**: You see this warning:\n```\nWARNING - Failed to fetch messages from channel random: Failed to fetch messages: {'code': -32603, 'message': 'users cache is not ready yet, sync process is still running... please wait'}\n```", "metadata": {}}
{"id": "168", "text": "## Common Issues and Solutions\n\n### Issue 1: \"users cache is not ready yet\" Error\n\n**Problem**: You see this warning:\n```\nWARNING - Failed to fetch messages from channel random: Failed to fetch messages: {'code': -32603, 'message': 'users cache is not ready yet, sync process is still running... please wait'}\n```\n\n**Solution**: This is a common timing issue. The LEANN integration now includes automatic retry logic:", "metadata": {}}
{"id": "169", "text": "## Common Issues and Solutions\n\n### Issue 1: \"users cache is not ready yet\" Error\n\n**Problem**: You see this warning:\n```\nWARNING - Failed to fetch messages from channel random: Failed to fetch messages: {'code': -32603, 'message': 'users cache is not ready yet, sync process is still running... please wait'}\n```\n\n**Solution**: This is a common timing issue. The LEANN integration now includes automatic retry logic:\n\n1. **Wait and Retry**: The system will automatically retry with exponential backoff (2s, 4s, 8s, etc.)\n2. **Increase Retry Parameters**: If needed, you can customize retry behavior:\n   ```bash\n   python -m apps.slack_rag \\\n     --mcp-server \"slack-mcp-server\" \\\n     --max-retries 10 \\\n     --retry-delay 3.0 \\\n     --channels general \\\n     --query \"Your query here\"\n   ```\n3. **Keep MCP Server Running**: Start the MCP server separately and keep it running:\n   ```bash\n   # Terminal 1: Start MCP server\n   slack-mcp-server", "metadata": {}}
{"id": "170", "text": "# Terminal 2: Run LEANN (it will connect to the running server)\n   python -m apps.slack_rag --mcp-server \"slack-mcp-server\" --channels general --query \"test\"\n   ```\n\n### Issue 2: \"No message fetching tool found\"\n\n**Problem**: The MCP server doesn't have the expected tools.\n\n**Solution**:\n1. Check if your MCP server is properly installed and configured\n2. Verify your Slack tokens are correct\n3. Try a different MCP server implementation\n4. Check the MCP server documentation for required configuration\n\n### Issue 3: Permission Denied Errors\n\n**Problem**: You get permission errors when trying to access channels.\n\n**Solutions**:\n1. **Check Bot Permissions**: Ensure your bot has been added to the channels you want to access\n2. **Verify Token Scopes**: Make sure you have all required scopes configured\n3. **Channel Access**: For private channels, the bot needs to be explicitly invited\n4. **Workspace Permissions**: Ensure your Slack app has the necessary workspace permissions\n\n### Issue 4: Empty Results", "metadata": {}}
{"id": "171", "text": "### Issue 3: Permission Denied Errors\n\n**Problem**: You get permission errors when trying to access channels.\n\n**Solutions**:\n1. **Check Bot Permissions**: Ensure your bot has been added to the channels you want to access\n2. **Verify Token Scopes**: Make sure you have all required scopes configured\n3. **Channel Access**: For private channels, the bot needs to be explicitly invited\n4. **Workspace Permissions**: Ensure your Slack app has the necessary workspace permissions\n\n### Issue 4: Empty Results\n\n**Problem**: No messages are returned even though the channel has messages.", "metadata": {}}
{"id": "172", "text": "### Issue 3: Permission Denied Errors\n\n**Problem**: You get permission errors when trying to access channels.\n\n**Solutions**:\n1. **Check Bot Permissions**: Ensure your bot has been added to the channels you want to access\n2. **Verify Token Scopes**: Make sure you have all required scopes configured\n3. **Channel Access**: For private channels, the bot needs to be explicitly invited\n4. **Workspace Permissions**: Ensure your Slack app has the necessary workspace permissions\n\n### Issue 4: Empty Results\n\n**Problem**: No messages are returned even though the channel has messages.\n\n**Solutions**:\n1. **Check Channel Names**: Ensure channel names are correct (without the # symbol)\n2. **Verify Bot Access**: Make sure the bot can access the channels\n3. **Check Date Ranges**: Some MCP servers have limitations on message history\n4. **Increase Message Limits**: Try increasing the message limit:\n   ```bash\n   python -m apps.slack_rag \\\n     --mcp-server \"slack-mcp-server\" \\\n     --channels general \\\n     --max-messages-per-channel 1000 \\\n     --query \"test\"\n   ```", "metadata": {}}
{"id": "173", "text": "**Solutions**:\n1. **Check Channel Names**: Ensure channel names are correct (without the # symbol)\n2. **Verify Bot Access**: Make sure the bot can access the channels\n3. **Check Date Ranges**: Some MCP servers have limitations on message history\n4. **Increase Message Limits**: Try increasing the message limit:\n   ```bash\n   python -m apps.slack_rag \\\n     --mcp-server \"slack-mcp-server\" \\\n     --channels general \\\n     --max-messages-per-channel 1000 \\\n     --query \"test\"\n   ```\n\n## Advanced Configuration\n\n### Custom MCP Server Commands\n\nIf you need to pass additional parameters to your MCP server:\n\n```bash\npython -m apps.slack_rag \\\n  --mcp-server \"slack-mcp-server --token-file /path/to/tokens.json\" \\\n  --workspace-name \"Your Workspace\" \\\n  --channels general \\\n  --query \"Your query\"\n```\n\n### Multiple Workspaces\n\nTo work with multiple Slack workspaces, you can:", "metadata": {}}
{"id": "174", "text": "## Advanced Configuration\n\n### Custom MCP Server Commands\n\nIf you need to pass additional parameters to your MCP server:\n\n```bash\npython -m apps.slack_rag \\\n  --mcp-server \"slack-mcp-server --token-file /path/to/tokens.json\" \\\n  --workspace-name \"Your Workspace\" \\\n  --channels general \\\n  --query \"Your query\"\n```\n\n### Multiple Workspaces\n\nTo work with multiple Slack workspaces, you can:\n\n1. Create separate apps for each workspace\n2. Use different environment variables\n3. Run separate instances with different configurations\n\n### Performance Optimization\n\nFor better performance with large workspaces:\n\n```bash\npython -m apps.slack_rag \\\n  --mcp-server \"slack-mcp-server\" \\\n  --workspace-name \"Your Workspace\" \\\n  --max-messages-per-channel 500 \\\n  --no-concatenate-conversations \\\n  --query \"Your query\"\n```\n---\n\n## Troubleshooting Checklist", "metadata": {}}
{"id": "175", "text": "### Multiple Workspaces\n\nTo work with multiple Slack workspaces, you can:\n\n1. Create separate apps for each workspace\n2. Use different environment variables\n3. Run separate instances with different configurations\n\n### Performance Optimization\n\nFor better performance with large workspaces:\n\n```bash\npython -m apps.slack_rag \\\n  --mcp-server \"slack-mcp-server\" \\\n  --workspace-name \"Your Workspace\" \\\n  --max-messages-per-channel 500 \\\n  --no-concatenate-conversations \\\n  --query \"Your query\"\n```\n---\n\n## Troubleshooting Checklist\n\n- [ ] Slack app created with proper permissions\n- [ ] Bot token (xoxb-) copied correctly\n- [ ] App-level token (xapp-) created if needed\n- [ ] MCP server installed and accessible\n- [ ] Ollama installed and running (`brew services start ollama`)\n- [ ] Ollama model pulled (`ollama pull llama3.2:1b`)\n- [ ] Environment variables set correctly\n- [ ] Bot invited to relevant channels\n- [ ] Channel names specified without # symbol\n- [ ] Sufficient retry attempts configured\n- [ ] Network connectivity to Slack APIs", "metadata": {}}
{"id": "176", "text": "- [ ] Slack app created with proper permissions\n- [ ] Bot token (xoxb-) copied correctly\n- [ ] App-level token (xapp-) created if needed\n- [ ] MCP server installed and accessible\n- [ ] Ollama installed and running (`brew services start ollama`)\n- [ ] Ollama model pulled (`ollama pull llama3.2:1b`)\n- [ ] Environment variables set correctly\n- [ ] Bot invited to relevant channels\n- [ ] Channel names specified without # symbol\n- [ ] Sufficient retry attempts configured\n- [ ] Network connectivity to Slack APIs\n\n## Getting Help\n\nIf you continue to have issues:", "metadata": {}}
{"id": "177", "text": "## Getting Help\n\nIf you continue to have issues:\n\n1. **Check Logs**: Look for detailed error messages in the console output\n2. **Test MCP Server**: Use `--test-connection` to verify the MCP server is working\n3. **Verify Tokens**: Double-check that your Slack tokens are valid and have the right scopes\n4. **Check Ollama**: Ensure Ollama is running (`ollama serve`) and the model is available (`ollama list`)\n5. **Community Support**: Reach out to the LEANN community for help\n\n## Example Commands\n\n### Basic Usage\n```bash\n# Test connection\npython -m apps.slack_rag --mcp-server \"slack-mcp-server\" --test-connection\n\n# Index specific channels\npython -m apps.slack_rag \\\n  --mcp-server \"slack-mcp-server\" \\\n  --workspace-name \"My Company\" \\\n  --channels general random \\\n  --query \"What did we decide about the project timeline?\"\n```", "metadata": {}}
{"id": "178", "text": "## Example Commands\n\n### Basic Usage\n```bash\n# Test connection\npython -m apps.slack_rag --mcp-server \"slack-mcp-server\" --test-connection\n\n# Index specific channels\npython -m apps.slack_rag \\\n  --mcp-server \"slack-mcp-server\" \\\n  --workspace-name \"My Company\" \\\n  --channels general random \\\n  --query \"What did we decide about the project timeline?\"\n```\n\n### Advanced Usage\n```bash\n# With custom retry settings\npython -m apps.slack_rag \\\n  --mcp-server \"slack-mcp-server\" \\\n  --workspace-name \"My Company\" \\\n  --channels general \\\n  --max-retries 10 \\\n  --retry-delay 5.0 \\\n  --max-messages-per-channel 2000 \\\n  --query \"Show me all decisions made in the last month\"\n```", "metadata": {}}
